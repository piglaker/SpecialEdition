Tue Feb 22 11:43:18 UTC 2022
Loading Dataset !
Using Large v2 as extra train set
  0%|          | 0/316634 [00:00<?, ?it/s]  3%|▎         | 10022/316634 [00:00<00:03, 100211.73it/s]  7%|▋         | 20636/316634 [00:00<00:02, 103692.20it/s] 10%|█         | 32886/316634 [00:00<00:02, 112274.69it/s] 14%|█▍        | 44604/316634 [00:00<00:02, 114207.31it/s] 18%|█▊        | 56386/316634 [00:00<00:02, 115504.97it/s] 22%|██▏       | 68601/316634 [00:00<00:02, 117761.88it/s] 25%|██▌       | 80378/316634 [00:00<00:02, 116119.49it/s] 29%|██▉       | 91995/316634 [00:00<00:01, 114085.18it/s] 33%|███▎      | 103413/316634 [00:00<00:01, 112975.01it/s] 36%|███▌      | 114726/316634 [00:01<00:01, 113019.00it/s] 40%|███▉      | 126033/316634 [00:01<00:01, 111798.69it/s] 43%|████▎     | 137340/316634 [00:01<00:01, 112176.21it/s] 47%|████▋     | 148594/316634 [00:01<00:01, 112282.78it/s] 50%|█████     | 159826/316634 [00:01<00:01, 111046.15it/s] 54%|█████▍    | 171106/316634 [00:01<00:01, 111564.50it/s] 58%|█████▊    | 182327/316634 [00:01<00:01, 110838.45it/s] 61%|██████    | 193414/316634 [00:03<00:07, 16401.51it/s]  65%|██████▍   | 204354/316634 [00:03<00:05, 21893.73it/s] 68%|██████▊   | 215522/316634 [00:03<00:03, 28879.38it/s] 72%|███████▏  | 226601/316634 [00:03<00:02, 37018.66it/s] 75%|███████▌  | 237976/316634 [00:04<00:01, 46595.50it/s] 79%|███████▉  | 249471/316634 [00:04<00:01, 56941.90it/s] 82%|████████▏ | 260495/316634 [00:04<00:00, 66417.42it/s] 86%|████████▌ | 271708/316634 [00:04<00:00, 75674.75it/s] 89%|████████▉ | 283026/316634 [00:04<00:00, 84089.50it/s] 93%|█████████▎| 294070/316634 [00:04<00:00, 89091.46it/s] 96%|█████████▋| 305404/316634 [00:04<00:00, 95261.49it/s]100%|██████████| 316634/316634 [00:04<00:00, 66381.13it/s]
  0%|          | 0/1000 [00:00<?, ?it/s]100%|██████████| 1000/1000 [00:00<00:00, 74413.27it/s]
  0%|          | 0/1000 [00:00<?, ?it/s]100%|██████████| 1000/1000 [00:00<00:00, 110761.17it/s]Tue Feb 22 11:44:12 UTC 2022

Some weights of the model checkpoint at fnlp/bart-large-chinese were not used when initializing BartForConditionalGeneration: ['encoder.layers.1.fc2.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.9.fc1.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.9.fc2.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.8.fc2.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.6.fc2.weight', 'encoder.layers.9.fc2.weight', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.7.fc2.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.8.fc1.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.2.fc2.weight', 'encoder.embed_tokens.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.embed_positions.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.9.fc1.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.10.fc1.weight', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.k_proj.weight']
- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at fnlp/bart-large-chinese and are newly initialized: ['encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.6.intermediate.dense.weight', 'encoder.encoder.layer.11.output.dense.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.output.dense.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.self.query.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.9.attention.output.dense.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.8.output.dense.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.8.attention.self.key.bias', 'encoder.encoder.layer.7.intermediate.dense.bias', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.8.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.self.query.weight', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.encoder.layer.8.intermediate.dense.bias', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.10.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp fp16 backend
***** Running training *****
  Num examples = 316634
  Num Epochs = 10
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 49480
Loading Succeed !
Hint: Loading Model *****BART-large*****
  0%|          | 0/49480 [00:00<?, ?it/s]/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  0%|          | 1/49480 [00:01<23:14:18,  1.69s/it]Traceback (most recent call last):
  File "proto_model.py", line 185, in <module>
    run()
  File "proto_model.py", line 122, in run
    train_result = trainer.train()
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1289, in train
    tr_loss += self.training_step(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1780, in training_step
    loss = self.compute_loss(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1814, in compute_loss
    outputs = model(**inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/models/bart/modeling_bart_v2.py", line 1280, in forward
    lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias
RuntimeError: CUDA out of memory. Tried to allocate 662.00 MiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 68.56 MiB free; 22.17 GiB reserved in total by PyTorch)
  0%|          | 1/49480 [00:03<53:43:29,  3.91s/it]