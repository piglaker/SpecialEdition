2021年 11月 12日 星期五 05:50:43 UTC
Loading Dataset !
  0%|          | 0/97500 [00:00<?, ?it/s] 11%|█         | 10483/97500 [00:00<00:00, 104726.48it/s] 22%|██▏       | 21758/97500 [00:00<00:00, 109438.66it/s] 34%|███▎      | 32804/97500 [00:00<00:00, 109897.66it/s] 45%|████▍     | 43794/97500 [00:00<00:01, 36124.13it/s]  59%|█████▉    | 57564/97500 [00:01<00:00, 52006.64it/s] 73%|███████▎  | 70978/97500 [00:01<00:00, 67002.93it/s] 87%|████████▋ | 84723/97500 [00:01<00:00, 81575.76it/s] 99%|█████████▉| 96581/97500 [00:01<00:00, 39822.63it/s]100%|██████████| 97500/97500 [00:01<00:00, 51973.94it/s]
  0%|          | 0/500 [00:00<?, ?it/s]100%|██████████| 500/500 [00:00<00:00, 79812.45it/s]
  0%|          | 0/2000 [00:00<?, ?it/s]100%|██████████| 2000/2000 [00:00<00:00, 130930.84it/s]2021年 11月 12日 星期五 05:51:07 UTC

Using amp fp16 backend
***** Running training *****
  Num examples = 97500
  Num Epochs = 10
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 256
  Gradient Accumulation steps = 1
  Total optimization steps = 3810
Loading Succeed !
  0%|          | 0/3810 [00:00<?, ?it/s]