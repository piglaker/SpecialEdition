Sat Oct  9 11:17:57 UTC 2021
Seq2SeqTrainingArguments(
_n_gpu=7,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=./tmp/tst-csc-test/runs/Oct09_11-17-47_07f9b83e0cba,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
output_dir=./tmp/tst-csc-test,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=tst-csc-test,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=./tmp/tst-csc-test,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loading Dataset !
  0%|          | 0/95000 [00:00<?, ?it/s] 14%|█▍        | 13272/95000 [00:00<00:00, 132705.57it/s] 28%|██▊       | 26657/95000 [00:00<00:00, 133370.21it/s] 42%|████▏     | 39995/95000 [00:00<00:01, 45471.32it/s]  57%|█████▋    | 53706/95000 [00:00<00:00, 62136.84it/s] 72%|███████▏  | 67971/95000 [00:00<00:00, 78742.01it/s] 86%|████████▋ | 82064/95000 [00:01<00:00, 92894.16it/s]100%|█████████▉| 94682/95000 [00:01<00:00, 43305.15it/s]100%|██████████| 95000/95000 [00:01<00:00, 56906.63it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]100%|██████████| 5000/5000 [00:00<00:00, 130075.30it/s]Sat Oct  9 11:18:18 UTC 2021

Using amp fp16 backend
***** Running training *****
  Num examples = 95000
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 112
  Gradient Accumulation steps = 1
  Total optimization steps = 8490
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-849
Configuration saved in ./tmp/tst-csc-test/checkpoint-849/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-849/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-849/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-849/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-1698
Configuration saved in ./tmp/tst-csc-test/checkpoint-1698/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-1698/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-1698/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-1698/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-2547
Configuration saved in ./tmp/tst-csc-test/checkpoint-2547/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-2547/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-2547/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-2547/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-3396
Configuration saved in ./tmp/tst-csc-test/checkpoint-3396/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-3396/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-3396/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-3396/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-4245
Configuration saved in ./tmp/tst-csc-test/checkpoint-4245/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-4245/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-4245/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-4245/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-5094
Configuration saved in ./tmp/tst-csc-test/checkpoint-5094/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-5094/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-5094/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-5094/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-5943
Configuration saved in ./tmp/tst-csc-test/checkpoint-5943/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-5943/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-5943/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-5943/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-6792
Configuration saved in ./tmp/tst-csc-test/checkpoint-6792/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-6792/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-6792/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-6792/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-7641
Configuration saved in ./tmp/tst-csc-test/checkpoint-7641/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-7641/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-7641/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-7641/special_tokens_map.json
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
Saving model checkpoint to ./tmp/tst-csc-test/checkpoint-8490
Configuration saved in ./tmp/tst-csc-test/checkpoint-8490/config.json
Model weights saved in ./tmp/tst-csc-test/checkpoint-8490/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/checkpoint-8490/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/checkpoint-8490/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tmp/tst-csc-test/checkpoint-3396 (score: 1.4846274852752686).
Saving model checkpoint to ./tmp/tst-csc-test
Configuration saved in ./tmp/tst-csc-test/config.json
Model weights saved in ./tmp/tst-csc-test/pytorch_model.bin
tokenizer config file saved in ./tmp/tst-csc-test/tokenizer_config.json
Special tokens file saved in ./tmp/tst-csc-test/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 56
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** Running Prediction *****
  Num examples = 5000
  Batch size = 56
/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
Loading Succeed !
{'loss': 2.592, 'learning_rate': 4.707891637220259e-05, 'epoch': 0.59}
{'eval_loss': 1.8162261247634888, 'eval_runtime': 49.8775, 'eval_samples_per_second': 100.246, 'eval_steps_per_second': 1.804, 'epoch': 1.0}
{'loss': 1.9764, 'learning_rate': 4.413427561837456e-05, 'epoch': 1.18}
{'loss': 1.7004, 'learning_rate': 4.118963486454653e-05, 'epoch': 1.77}
{'eval_loss': 1.5749397277832031, 'eval_runtime': 50.3865, 'eval_samples_per_second': 99.233, 'eval_steps_per_second': 1.786, 'epoch': 2.0}
{'loss': 1.4913, 'learning_rate': 3.8244994110718496e-05, 'epoch': 2.36}
{'loss': 1.3778, 'learning_rate': 3.530035335689046e-05, 'epoch': 2.94}
{'eval_loss': 1.5086547136306763, 'eval_runtime': 50.6514, 'eval_samples_per_second': 98.714, 'eval_steps_per_second': 1.777, 'epoch': 3.0}
{'loss': 1.2003, 'learning_rate': 3.2355712603062426e-05, 'epoch': 3.53}
{'eval_loss': 1.4846274852752686, 'eval_runtime': 50.7663, 'eval_samples_per_second': 98.49, 'eval_steps_per_second': 1.773, 'epoch': 4.0}
{'loss': 1.1439, 'learning_rate': 2.941696113074205e-05, 'epoch': 4.12}
{'loss': 1.0292, 'learning_rate': 2.6472320376914017e-05, 'epoch': 4.71}
{'eval_loss': 1.5068048238754272, 'eval_runtime': 50.5988, 'eval_samples_per_second': 98.816, 'eval_steps_per_second': 1.779, 'epoch': 5.0}
{'loss': 0.9719, 'learning_rate': 2.3527679623085985e-05, 'epoch': 5.3}
{'loss': 0.9135, 'learning_rate': 2.058303886925795e-05, 'epoch': 5.89}
{'eval_loss': 1.5101416110992432, 'eval_runtime': 50.7556, 'eval_samples_per_second': 98.511, 'eval_steps_per_second': 1.773, 'epoch': 6.0}
{'loss': 0.8306, 'learning_rate': 1.7638398115429918e-05, 'epoch': 6.48}
{'eval_loss': 1.525174856185913, 'eval_runtime': 50.6238, 'eval_samples_per_second': 98.768, 'eval_steps_per_second': 1.778, 'epoch': 7.0}
{'loss': 0.8134, 'learning_rate': 1.4693757361601884e-05, 'epoch': 7.07}
{'loss': 0.749, 'learning_rate': 1.1755005889281508e-05, 'epoch': 7.66}
{'eval_loss': 1.5614553689956665, 'eval_runtime': 50.5884, 'eval_samples_per_second': 98.837, 'eval_steps_per_second': 1.779, 'epoch': 8.0}
{'loss': 0.7239, 'learning_rate': 8.810365135453474e-06, 'epoch': 8.24}
{'loss': 0.6929, 'learning_rate': 5.865724381625442e-06, 'epoch': 8.83}
{'eval_loss': 1.5785977840423584, 'eval_runtime': 50.9786, 'eval_samples_per_second': 98.08, 'eval_steps_per_second': 1.765, 'epoch': 9.0}
{'loss': 0.6616, 'learning_rate': 2.921083627797409e-06, 'epoch': 9.42}
{'eval_loss': 1.5927828550338745, 'eval_runtime': 50.7647, 'eval_samples_per_second': 98.494, 'eval_steps_per_second': 1.773, 'epoch': 10.0}
{'train_runtime': 13056.4084, 'train_samples_per_second': 72.761, 'train_steps_per_second': 0.65, 'train_loss': 1.1491747026027863, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     1.1492
  train_runtime            = 3:37:36.40
  train_samples            =      95000
  train_samples_per_second =     72.761
  train_steps_per_second   =       0.65
{'eval_loss': 1.4846274852752686, 'eval_runtime': 51.127, 'eval_samples_per_second': 97.796, 'eval_steps_per_second': 1.76, 'epoch': 10.0}
***** eval metrics *****
  epoch                   =       10.0
  eval_loss               =     1.4846
  eval_runtime            = 0:00:51.12
  eval_samples            =       5000
  eval_samples_per_second =     97.796
  eval_steps_per_second   =       1.76
