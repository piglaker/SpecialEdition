Sun Dec 19 04:50:27 UTC 2021
Loading Dataset !
Loading Expanded Abs_Pos Bert SigHan Dataset ...
  0%|          | 0/1000 [00:00<?, ?it/s]100%|██████████| 1000/1000 [00:00<00:00, 151298.75it/s]
  0%|          | 0/600 [00:00<?, ?it/s]100%|██████████| 600/600 [00:00<00:00, 181584.70it/s]
  0%|          | 0/1100 [00:00<?, ?it/s]100%|██████████| 1100/1100 [00:00<00:00, 176271.66it/s]Sun Dec 19 04:50:39 UTC 2021

Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM_v2: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using amp fp16 backend
***** Running training *****
  Num examples = 1000
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 320
Save cache to cache/sighan_abs_pos_plus.
Loading Succeed !
  0%|          | 0/320 [00:00<?, ?it/s]/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  0%|          | 1/320 [00:01<05:26,  1.02s/it]  1%|          | 2/320 [00:01<02:41,  1.96it/s]  1%|          | 3/320 [00:01<01:53,  2.78it/s]  1%|▏         | 4/320 [00:01<01:25,  3.70it/s]  2%|▏         | 5/320 [00:01<01:11,  4.41it/s]  2%|▏         | 6/320 [00:01<01:03,  4.94it/s]  2%|▏         | 7/320 [00:01<00:57,  5.46it/s]  2%|▎         | 8/320 [00:02<01:00,  5.13it/s]  3%|▎         | 9/320 [00:02<00:58,  5.31it/s]  3%|▎         | 10/320 [00:02<00:55,  5.58it/s]  3%|▎         | 11/320 [00:02<00:52,  5.93it/s]  4%|▍         | 12/320 [00:02<00:49,  6.22it/s]  4%|▍         | 13/320 [00:02<00:48,  6.30it/s]  4%|▍         | 14/320 [00:03<00:48,  6.30it/s]  5%|▍         | 15/320 [00:03<00:49,  6.11it/s]  5%|▌         | 16/320 [00:03<00:49,  6.16it/s]  5%|▌         | 17/320 [00:03<00:46,  6.46it/s]  6%|▌         | 18/320 [00:03<00:46,  6.55it/s]  6%|▌         | 19/320 [00:03<00:44,  6.80it/s]  6%|▋         | 20/320 [00:04<00:45,  6.61it/s]  7%|▋         | 21/320 [00:04<00:48,  6.22it/s]  7%|▋         | 22/320 [00:04<00:47,  6.29it/s]  7%|▋         | 23/320 [00:04<00:44,  6.61it/s]  8%|▊         | 24/320 [00:04<00:48,  6.15it/s]  8%|▊         | 25/320 [00:04<00:45,  6.49it/s]  8%|▊         | 26/320 [00:04<00:45,  6.45it/s]  8%|▊         | 27/320 [00:05<00:48,  6.03it/s]  9%|▉         | 28/320 [00:05<00:48,  6.02it/s]  9%|▉         | 29/320 [00:05<00:48,  6.02it/s]  9%|▉         | 30/320 [00:05<00:49,  5.83it/s] 10%|▉         | 31/320 [00:05<00:48,  5.91it/s]
  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.12it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.01it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.01it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 21.67it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.28it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.64it/s][A                                                
                                               [A 10%|█         | 32/320 [00:06<00:48,  5.91it/s]
100%|██████████| 19/19 [00:00<00:00, 18.64it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-32
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-32/special_tokens_map.json
 10%|█         | 33/320 [00:11<06:14,  1.30s/it] 11%|█         | 34/320 [00:11<04:51,  1.02s/it] 11%|█         | 35/320 [00:11<03:51,  1.23it/s] 11%|█▏        | 36/320 [00:11<02:58,  1.59it/s] 12%|█▏        | 37/320 [00:11<02:21,  2.00it/s] 12%|█▏        | 38/320 [00:11<01:52,  2.50it/s] 12%|█▏        | 39/320 [00:12<01:34,  2.98it/s] 12%|█▎        | 40/320 [00:12<01:20,  3.49it/s] 13%|█▎        | 41/320 [00:12<01:10,  3.96it/s] 13%|█▎        | 42/320 [00:12<01:01,  4.49it/s] 13%|█▎        | 43/320 [00:12<00:59,  4.67it/s] 14%|█▍        | 44/320 [00:12<00:53,  5.13it/s] 14%|█▍        | 45/320 [00:13<00:50,  5.43it/s] 14%|█▍        | 46/320 [00:13<00:50,  5.38it/s] 15%|█▍        | 47/320 [00:13<00:47,  5.75it/s] 15%|█▌        | 48/320 [00:13<00:45,  5.99it/s] 15%|█▌        | 49/320 [00:13<00:43,  6.19it/s] 16%|█▌        | 50/320 [00:13<00:42,  6.39it/s] 16%|█▌        | 51/320 [00:14<00:41,  6.47it/s] 16%|█▋        | 52/320 [00:14<00:40,  6.56it/s] 17%|█▋        | 53/320 [00:14<00:39,  6.82it/s] 17%|█▋        | 54/320 [00:14<00:42,  6.31it/s] 17%|█▋        | 55/320 [00:14<00:41,  6.43it/s] 18%|█▊        | 56/320 [00:14<00:40,  6.56it/s] 18%|█▊        | 57/320 [00:14<00:39,  6.65it/s] 18%|█▊        | 58/320 [00:15<00:38,  6.77it/s] 18%|█▊        | 59/320 [00:15<00:40,  6.41it/s] 19%|█▉        | 60/320 [00:15<00:40,  6.36it/s] 19%|█▉        | 61/320 [00:15<00:41,  6.23it/s] 19%|█▉        | 62/320 [00:15<00:40,  6.30it/s] 20%|█▉        | 63/320 [00:15<00:40,  6.30it/s]{'eval_loss': 0.13109168410301208, 'eval_F1_score': 0.1724137930560828, 'eval_Precision': 0.22471910112346927, 'eval_Recall': 0.13986013986009097, 'eval_Metric_time': 0.017113447189331055, 'eval_runtime': 1.0161, 'eval_samples_per_second': 590.511, 'eval_steps_per_second': 18.7, 'epoch': 1.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 26.08it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.79it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.90it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.24it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.63it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.90it/s][A                                                
                                               [A 20%|██        | 64/320 [00:16<00:40,  6.30it/s]
100%|██████████| 19/19 [00:00<00:00, 18.90it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-64
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-64/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-64/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-64/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-64/special_tokens_map.json
 20%|██        | 65/320 [00:21<05:29,  1.29s/it] 21%|██        | 66/320 [00:21<04:19,  1.02s/it] 21%|██        | 67/320 [00:21<03:20,  1.26it/s] 21%|██▏       | 68/320 [00:21<02:37,  1.60it/s] 22%|██▏       | 69/320 [00:21<02:03,  2.03it/s] 22%|██▏       | 70/320 [00:21<01:38,  2.54it/s] 22%|██▏       | 71/320 [00:22<01:21,  3.07it/s] 22%|██▎       | 72/320 [00:22<01:11,  3.48it/s] 23%|██▎       | 73/320 [00:22<01:01,  4.02it/s] 23%|██▎       | 74/320 [00:22<00:53,  4.58it/s] 23%|██▎       | 75/320 [00:22<00:50,  4.85it/s] 24%|██▍       | 76/320 [00:22<00:47,  5.14it/s] 24%|██▍       | 77/320 [00:23<00:43,  5.56it/s] 24%|██▍       | 78/320 [00:23<00:43,  5.61it/s] 25%|██▍       | 79/320 [00:23<00:42,  5.70it/s] 25%|██▌       | 80/320 [00:23<00:40,  5.86it/s] 25%|██▌       | 81/320 [00:23<00:42,  5.66it/s] 26%|██▌       | 82/320 [00:23<00:41,  5.74it/s] 26%|██▌       | 83/320 [00:24<00:44,  5.31it/s] 26%|██▋       | 84/320 [00:24<00:42,  5.59it/s] 27%|██▋       | 85/320 [00:24<00:40,  5.78it/s] 27%|██▋       | 86/320 [00:24<00:38,  6.01it/s] 27%|██▋       | 87/320 [00:24<00:38,  6.03it/s] 28%|██▊       | 88/320 [00:24<00:37,  6.14it/s] 28%|██▊       | 89/320 [00:25<00:36,  6.37it/s] 28%|██▊       | 90/320 [00:25<00:36,  6.34it/s] 28%|██▊       | 91/320 [00:25<00:34,  6.59it/s] 29%|██▉       | 92/320 [00:25<00:37,  6.12it/s] 29%|██▉       | 93/320 [00:25<00:35,  6.31it/s] 29%|██▉       | 94/320 [00:25<00:35,  6.41it/s] 30%|██▉       | 95/320 [00:26<00:35,  6.37it/s] 30%|███       | 96/320 [00:26<00:33,  6.59it/s]{'eval_loss': 0.11963073909282684, 'eval_F1_score': 0.26180257505977084, 'eval_Precision': 0.33888888888870067, 'eval_Recall': 0.21328671328663873, 'eval_Metric_time': 0.018462181091308594, 'eval_runtime': 0.9969, 'eval_samples_per_second': 601.884, 'eval_steps_per_second': 19.06, 'epoch': 2.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.52it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.79it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 22.00it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.48it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.99it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 19.21it/s][A                                                
                                               [A 30%|███       | 96/320 [00:27<00:33,  6.59it/s]
100%|██████████| 19/19 [00:00<00:00, 19.21it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-96
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-96/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-96/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-96/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-96/special_tokens_map.json
 30%|███       | 97/320 [00:31<06:09,  1.66s/it] 31%|███       | 98/320 [00:31<04:29,  1.21s/it] 31%|███       | 99/320 [00:31<03:18,  1.11it/s] 31%|███▏      | 100/320 [00:31<02:27,  1.49it/s] 32%|███▏      | 101/320 [00:31<01:53,  1.93it/s] 32%|███▏      | 102/320 [00:32<01:27,  2.49it/s] 32%|███▏      | 103/320 [00:32<01:12,  3.00it/s] 32%|███▎      | 104/320 [00:32<01:00,  3.59it/s] 33%|███▎      | 105/320 [00:32<00:53,  4.06it/s] 33%|███▎      | 106/320 [00:32<00:46,  4.58it/s] 33%|███▎      | 107/320 [00:32<00:43,  4.87it/s] 34%|███▍      | 108/320 [00:33<00:40,  5.25it/s] 34%|███▍      | 109/320 [00:33<00:37,  5.62it/s] 34%|███▍      | 110/320 [00:33<00:34,  6.06it/s] 35%|███▍      | 111/320 [00:33<00:35,  5.95it/s] 35%|███▌      | 112/320 [00:33<00:35,  5.83it/s] 35%|███▌      | 113/320 [00:33<00:33,  6.14it/s] 36%|███▌      | 114/320 [00:34<00:33,  6.23it/s] 36%|███▌      | 115/320 [00:34<00:32,  6.26it/s] 36%|███▋      | 116/320 [00:34<00:31,  6.56it/s] 37%|███▋      | 117/320 [00:34<00:31,  6.50it/s] 37%|███▋      | 118/320 [00:34<00:29,  6.75it/s] 37%|███▋      | 119/320 [00:34<00:31,  6.48it/s] 38%|███▊      | 120/320 [00:34<00:30,  6.51it/s] 38%|███▊      | 121/320 [00:35<00:32,  6.08it/s] 38%|███▊      | 122/320 [00:35<00:31,  6.26it/s] 38%|███▊      | 123/320 [00:35<00:32,  6.15it/s] 39%|███▉      | 124/320 [00:35<00:33,  5.89it/s] 39%|███▉      | 125/320 [00:35<00:36,  5.36it/s] 39%|███▉      | 126/320 [00:36<00:35,  5.53it/s] 40%|███▉      | 127/320 [00:36<00:35,  5.42it/s]{'eval_loss': 0.12311173975467682, 'eval_F1_score': 0.3168724279349744, 'eval_Precision': 0.3849999999998075, 'eval_Recall': 0.2692307692306751, 'eval_Metric_time': 0.014870643615722656, 'eval_runtime': 0.9854, 'eval_samples_per_second': 608.89, 'eval_steps_per_second': 19.282, 'epoch': 3.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.77it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.78it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.76it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.12it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.61it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.87it/s][A                                                 
                                               [A 40%|████      | 128/320 [00:37<00:35,  5.42it/s]
100%|██████████| 19/19 [00:00<00:00, 18.87it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-128
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-128/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-128/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-128/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-128/special_tokens_map.json
 40%|████      | 129/320 [00:41<04:08,  1.30s/it] 41%|████      | 130/320 [00:41<03:15,  1.03s/it] 41%|████      | 131/320 [00:41<02:30,  1.26it/s] 41%|████▏     | 132/320 [00:41<01:56,  1.62it/s] 42%|████▏     | 133/320 [00:42<01:31,  2.04it/s] 42%|████▏     | 134/320 [00:42<01:13,  2.51it/s] 42%|████▏     | 135/320 [00:42<01:01,  3.01it/s] 42%|████▎     | 136/320 [00:42<00:53,  3.43it/s] 43%|████▎     | 137/320 [00:42<00:44,  4.08it/s] 43%|████▎     | 138/320 [00:42<00:38,  4.77it/s] 43%|████▎     | 139/320 [00:43<00:34,  5.19it/s] 44%|████▍     | 140/320 [00:43<00:31,  5.68it/s] 44%|████▍     | 141/320 [00:43<00:30,  5.89it/s] 44%|████▍     | 142/320 [00:43<00:30,  5.85it/s] 45%|████▍     | 143/320 [00:43<00:28,  6.14it/s] 45%|████▌     | 144/320 [00:43<00:29,  6.06it/s] 45%|████▌     | 145/320 [00:44<00:31,  5.49it/s] 46%|████▌     | 146/320 [00:44<00:30,  5.78it/s] 46%|████▌     | 147/320 [00:44<00:29,  5.84it/s] 46%|████▋     | 148/320 [00:44<00:29,  5.89it/s] 47%|████▋     | 149/320 [00:44<00:27,  6.25it/s] 47%|████▋     | 150/320 [00:44<00:28,  5.91it/s] 47%|████▋     | 151/320 [00:45<00:28,  5.88it/s] 48%|████▊     | 152/320 [00:45<00:28,  6.00it/s] 48%|████▊     | 153/320 [00:45<00:26,  6.30it/s] 48%|████▊     | 154/320 [00:45<00:25,  6.41it/s] 48%|████▊     | 155/320 [00:45<00:25,  6.35it/s] 49%|████▉     | 156/320 [00:45<00:25,  6.35it/s] 49%|████▉     | 157/320 [00:45<00:24,  6.55it/s] 49%|████▉     | 158/320 [00:46<00:23,  6.82it/s] 50%|████▉     | 159/320 [00:46<00:24,  6.65it/s]{'eval_loss': 0.13928410410881042, 'eval_F1_score': 0.3217391303876068, 'eval_Precision': 0.4252873563215947, 'eval_Recall': 0.25874125874116827, 'eval_Metric_time': 0.01740717887878418, 'eval_runtime': 0.9992, 'eval_samples_per_second': 600.484, 'eval_steps_per_second': 19.015, 'epoch': 4.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.73it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.25it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.61it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.00it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.55it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.85it/s][A                                                 
                                               [A 50%|█████     | 160/320 [00:47<00:24,  6.65it/s]
100%|██████████| 19/19 [00:00<00:00, 18.85it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-160
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-160/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-160/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-160/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-160/special_tokens_map.json
 50%|█████     | 161/320 [00:51<03:21,  1.26s/it] 51%|█████     | 162/320 [00:51<02:36,  1.01it/s] 51%|█████     | 163/320 [00:51<01:59,  1.31it/s] 51%|█████▏    | 164/320 [00:51<01:33,  1.68it/s] 52%|█████▏    | 165/320 [00:51<01:15,  2.05it/s] 52%|█████▏    | 166/320 [00:52<00:59,  2.58it/s] 52%|█████▏    | 167/320 [00:52<00:49,  3.09it/s] 52%|█████▎    | 168/320 [00:52<00:40,  3.73it/s] 53%|█████▎    | 169/320 [00:52<00:35,  4.25it/s] 53%|█████▎    | 170/320 [00:52<00:30,  4.91it/s] 53%|█████▎    | 171/320 [00:52<00:29,  5.10it/s] 54%|█████▍    | 172/320 [00:53<00:26,  5.53it/s] 54%|█████▍    | 173/320 [00:53<00:24,  5.98it/s] 54%|█████▍    | 174/320 [00:53<00:23,  6.12it/s] 55%|█████▍    | 175/320 [00:53<00:22,  6.49it/s] 55%|█████▌    | 176/320 [00:53<00:21,  6.55it/s] 55%|█████▌    | 177/320 [00:53<00:22,  6.30it/s] 56%|█████▌    | 178/320 [00:53<00:22,  6.36it/s] 56%|█████▌    | 179/320 [00:54<00:20,  6.87it/s] 56%|█████▋    | 180/320 [00:54<00:20,  6.88it/s] 57%|█████▋    | 181/320 [00:54<00:20,  6.88it/s] 57%|█████▋    | 182/320 [00:54<00:20,  6.86it/s] 57%|█████▋    | 183/320 [00:54<00:19,  6.92it/s] 57%|█████▊    | 184/320 [00:54<00:18,  7.21it/s] 58%|█████▊    | 185/320 [00:54<00:19,  6.78it/s] 58%|█████▊    | 186/320 [00:55<00:21,  6.23it/s] 58%|█████▊    | 187/320 [00:55<00:21,  6.28it/s] 59%|█████▉    | 188/320 [00:55<00:20,  6.49it/s] 59%|█████▉    | 189/320 [00:55<00:20,  6.31it/s] 59%|█████▉    | 190/320 [00:55<00:21,  6.05it/s] 60%|█████▉    | 191/320 [00:55<00:21,  6.05it/s]{'eval_loss': 0.14125394821166992, 'eval_F1_score': 0.33403805492033667, 'eval_Precision': 0.42245989304790244, 'eval_Recall': 0.27622377622367966, 'eval_Metric_time': 0.018524169921875, 'eval_runtime': 1.0032, 'eval_samples_per_second': 598.081, 'eval_steps_per_second': 18.939, 'epoch': 5.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 26.47it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 23.32it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 22.37it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.72it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 21.11it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 19.27it/s][A                                                 
                                               [A 60%|██████    | 192/320 [00:56<00:21,  6.05it/s]
100%|██████████| 19/19 [00:00<00:00, 19.27it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-192
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-192/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-192/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-192/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-192/special_tokens_map.json
 60%|██████    | 193/320 [01:01<02:41,  1.27s/it] 61%|██████    | 194/320 [01:01<02:06,  1.00s/it] 61%|██████    | 195/320 [01:01<01:38,  1.27it/s] 61%|██████▏   | 196/320 [01:01<01:16,  1.62it/s] 62%|██████▏   | 197/320 [01:01<00:59,  2.06it/s] 62%|██████▏   | 198/320 [01:01<00:47,  2.56it/s] 62%|██████▏   | 199/320 [01:02<00:39,  3.07it/s] 62%|██████▎   | 200/320 [01:02<00:33,  3.56it/s] 63%|██████▎   | 201/320 [01:02<00:31,  3.78it/s] 63%|██████▎   | 202/320 [01:02<00:27,  4.30it/s] 63%|██████▎   | 203/320 [01:02<00:25,  4.57it/s] 64%|██████▍   | 204/320 [01:02<00:23,  5.04it/s] 64%|██████▍   | 205/320 [01:03<00:21,  5.36it/s] 64%|██████▍   | 206/320 [01:03<00:20,  5.55it/s] 65%|██████▍   | 207/320 [01:03<00:18,  5.99it/s] 65%|██████▌   | 208/320 [01:03<00:17,  6.22it/s] 65%|██████▌   | 209/320 [01:03<00:17,  6.34it/s] 66%|██████▌   | 210/320 [01:03<00:17,  6.37it/s] 66%|██████▌   | 211/320 [01:03<00:16,  6.56it/s] 66%|██████▋   | 212/320 [01:04<00:16,  6.52it/s] 67%|██████▋   | 213/320 [01:04<00:17,  6.10it/s] 67%|██████▋   | 214/320 [01:04<00:16,  6.32it/s] 67%|██████▋   | 215/320 [01:04<00:16,  6.33it/s] 68%|██████▊   | 216/320 [01:04<00:16,  6.14it/s] 68%|██████▊   | 217/320 [01:04<00:15,  6.49it/s] 68%|██████▊   | 218/320 [01:05<00:16,  6.33it/s] 68%|██████▊   | 219/320 [01:05<00:15,  6.49it/s] 69%|██████▉   | 220/320 [01:05<00:15,  6.54it/s] 69%|██████▉   | 221/320 [01:05<00:14,  6.74it/s] 69%|██████▉   | 222/320 [01:05<00:14,  6.89it/s] 70%|██████▉   | 223/320 [01:05<00:15,  6.23it/s]{'eval_loss': 0.1382492333650589, 'eval_F1_score': 0.2881002087200317, 'eval_Precision': 0.35751295336769046, 'eval_Recall': 0.2412587412586569, 'eval_Metric_time': 0.015006542205810547, 'eval_runtime': 0.9718, 'eval_samples_per_second': 617.434, 'eval_steps_per_second': 19.552, 'epoch': 6.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.77it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.74it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.73it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 21.98it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.49it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.78it/s][A                                                 
                                               [A 70%|███████   | 224/320 [01:06<00:15,  6.23it/s]
100%|██████████| 19/19 [00:00<00:00, 18.78it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-224
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-224/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-224/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-224/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-224/special_tokens_map.json
 70%|███████   | 225/320 [01:10<01:59,  1.26s/it] 71%|███████   | 226/320 [01:11<01:32,  1.01it/s] 71%|███████   | 227/320 [01:11<01:11,  1.30it/s] 71%|███████▏  | 228/320 [01:11<00:55,  1.65it/s] 72%|███████▏  | 229/320 [01:11<00:43,  2.09it/s] 72%|███████▏  | 230/320 [01:11<00:34,  2.63it/s] 72%|███████▏  | 231/320 [01:11<00:28,  3.14it/s] 72%|███████▎  | 232/320 [01:12<00:23,  3.69it/s] 73%|███████▎  | 233/320 [01:12<00:20,  4.28it/s] 73%|███████▎  | 234/320 [01:12<00:17,  4.89it/s] 73%|███████▎  | 235/320 [01:12<00:15,  5.36it/s] 74%|███████▍  | 236/320 [01:12<00:14,  5.65it/s] 74%|███████▍  | 237/320 [01:12<00:13,  5.93it/s] 74%|███████▍  | 238/320 [01:12<00:13,  6.15it/s] 75%|███████▍  | 239/320 [01:13<00:13,  5.93it/s] 75%|███████▌  | 240/320 [01:13<00:13,  5.91it/s] 75%|███████▌  | 241/320 [01:13<00:13,  5.66it/s] 76%|███████▌  | 242/320 [01:13<00:13,  5.99it/s] 76%|███████▌  | 243/320 [01:13<00:12,  5.97it/s] 76%|███████▋  | 244/320 [01:13<00:12,  5.88it/s] 77%|███████▋  | 245/320 [01:14<00:12,  6.06it/s] 77%|███████▋  | 246/320 [01:14<00:11,  6.43it/s] 77%|███████▋  | 247/320 [01:14<00:12,  6.05it/s] 78%|███████▊  | 248/320 [01:14<00:13,  5.49it/s] 78%|███████▊  | 249/320 [01:14<00:12,  5.74it/s] 78%|███████▊  | 250/320 [01:14<00:11,  6.02it/s] 78%|███████▊  | 251/320 [01:15<00:12,  5.75it/s] 79%|███████▉  | 252/320 [01:15<00:11,  6.14it/s] 79%|███████▉  | 253/320 [01:15<00:10,  6.47it/s] 79%|███████▉  | 254/320 [01:15<00:10,  6.16it/s] 80%|███████▉  | 255/320 [01:15<00:10,  6.27it/s] 80%|████████  | 256/320 [01:15<00:09,  6.99it/s]{'eval_loss': 0.15065130591392517, 'eval_F1_score': 0.3383297644063442, 'eval_Precision': 0.43646408839754897, 'eval_Recall': 0.27622377622367966, 'eval_Metric_time': 0.01682877540588379, 'eval_runtime': 1.0012, 'eval_samples_per_second': 599.277, 'eval_steps_per_second': 18.977, 'epoch': 7.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.30it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 22.17it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.18it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 21.33it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 19.85it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.33it/s][A                                                 
                                               [A 80%|████████  | 256/320 [01:16<00:09,  6.99it/s]
100%|██████████| 19/19 [00:00<00:00, 18.33it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256/special_tokens_map.json
 80%|████████  | 257/320 [01:20<01:41,  1.61s/it] 81%|████████  | 258/320 [01:21<01:13,  1.18s/it] 81%|████████  | 259/320 [01:21<00:52,  1.15it/s] 81%|████████▏ | 260/320 [01:21<00:39,  1.53it/s] 82%|████████▏ | 261/320 [01:21<00:29,  1.98it/s] 82%|████████▏ | 262/320 [01:21<00:23,  2.51it/s] 82%|████████▏ | 263/320 [01:21<00:18,  3.11it/s] 82%|████████▎ | 264/320 [01:21<00:15,  3.72it/s] 83%|████████▎ | 265/320 [01:22<00:13,  4.22it/s] 83%|████████▎ | 266/320 [01:22<00:11,  4.80it/s] 83%|████████▎ | 267/320 [01:22<00:10,  5.26it/s] 84%|████████▍ | 268/320 [01:22<00:09,  5.52it/s] 84%|████████▍ | 269/320 [01:22<00:09,  5.62it/s] 84%|████████▍ | 270/320 [01:22<00:08,  5.76it/s] 85%|████████▍ | 271/320 [01:23<00:08,  6.06it/s] 85%|████████▌ | 272/320 [01:23<00:08,  5.98it/s] 85%|████████▌ | 273/320 [01:23<00:08,  5.79it/s] 86%|████████▌ | 274/320 [01:23<00:07,  5.84it/s] 86%|████████▌ | 275/320 [01:23<00:07,  5.86it/s] 86%|████████▋ | 276/320 [01:23<00:07,  6.06it/s] 87%|████████▋ | 277/320 [01:24<00:07,  5.54it/s] 87%|████████▋ | 278/320 [01:24<00:07,  5.46it/s] 87%|████████▋ | 279/320 [01:24<00:07,  5.77it/s] 88%|████████▊ | 280/320 [01:24<00:06,  6.04it/s] 88%|████████▊ | 281/320 [01:24<00:06,  6.10it/s] 88%|████████▊ | 282/320 [01:24<00:06,  6.16it/s] 88%|████████▊ | 283/320 [01:25<00:05,  6.47it/s] 89%|████████▉ | 284/320 [01:25<00:05,  6.21it/s] 89%|████████▉ | 285/320 [01:25<00:05,  6.36it/s] 89%|████████▉ | 286/320 [01:25<00:05,  6.35it/s] 90%|████████▉ | 287/320 [01:25<00:05,  6.45it/s]{'eval_loss': 0.16612432897090912, 'eval_F1_score': 0.3537117903459564, 'eval_Precision': 0.47093023255786576, 'eval_Recall': 0.2832167832166842, 'eval_Metric_time': 0.016814708709716797, 'eval_runtime': 1.0259, 'eval_samples_per_second': 584.853, 'eval_steps_per_second': 18.52, 'epoch': 8.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 25.91it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 21.83it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 21.29it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 21.80it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 20.40it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 18.76it/s][A                                                 
                                               [A 90%|█████████ | 288/320 [01:26<00:04,  6.45it/s]
100%|██████████| 19/19 [00:00<00:00, 18.76it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-288
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-288/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-288/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-288/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-288/special_tokens_map.json
 90%|█████████ | 289/320 [01:30<00:39,  1.27s/it] 91%|█████████ | 290/320 [01:31<00:30,  1.00s/it] 91%|█████████ | 291/320 [01:31<00:22,  1.29it/s] 91%|█████████▏| 292/320 [01:31<00:17,  1.63it/s] 92%|█████████▏| 293/320 [01:31<00:13,  2.07it/s] 92%|█████████▏| 294/320 [01:31<00:10,  2.47it/s] 92%|█████████▏| 295/320 [01:31<00:08,  3.08it/s] 92%|█████████▎| 296/320 [01:31<00:06,  3.67it/s] 93%|█████████▎| 297/320 [01:32<00:05,  4.19it/s] 93%|█████████▎| 298/320 [01:32<00:04,  4.73it/s] 93%|█████████▎| 299/320 [01:32<00:04,  5.09it/s] 94%|█████████▍| 300/320 [01:32<00:03,  5.62it/s] 94%|█████████▍| 301/320 [01:32<00:03,  5.81it/s] 94%|█████████▍| 302/320 [01:32<00:02,  6.17it/s] 95%|█████████▍| 303/320 [01:32<00:02,  6.69it/s] 95%|█████████▌| 304/320 [01:33<00:02,  6.83it/s] 95%|█████████▌| 305/320 [01:33<00:02,  6.89it/s] 96%|█████████▌| 306/320 [01:33<00:02,  6.68it/s] 96%|█████████▌| 307/320 [01:33<00:01,  6.82it/s] 96%|█████████▋| 308/320 [01:33<00:01,  7.02it/s] 97%|█████████▋| 309/320 [01:33<00:01,  7.00it/s] 97%|█████████▋| 310/320 [01:33<00:01,  6.98it/s] 97%|█████████▋| 311/320 [01:34<00:01,  6.84it/s] 98%|█████████▊| 312/320 [01:34<00:01,  6.85it/s] 98%|█████████▊| 313/320 [01:34<00:01,  6.90it/s] 98%|█████████▊| 314/320 [01:34<00:00,  6.90it/s] 98%|█████████▊| 315/320 [01:34<00:00,  6.47it/s] 99%|█████████▉| 316/320 [01:34<00:00,  6.50it/s] 99%|█████████▉| 317/320 [01:35<00:00,  6.62it/s] 99%|█████████▉| 318/320 [01:35<00:00,  6.28it/s]100%|█████████▉| 319/320 [01:35<00:00,  6.58it/s]{'eval_loss': 0.16420115530490875, 'eval_F1_score': 0.3199999999535328, 'eval_Precision': 0.43902439024363477, 'eval_Recall': 0.25174825174816373, 'eval_Metric_time': 0.016753673553466797, 'eval_runtime': 1.0072, 'eval_samples_per_second': 595.699, 'eval_steps_per_second': 18.864, 'epoch': 9.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|█▌        | 3/19 [00:00<00:00, 26.28it/s][A
 32%|███▏      | 6/19 [00:00<00:00, 23.06it/s][A
 47%|████▋     | 9/19 [00:00<00:00, 22.23it/s][A
 63%|██████▎   | 12/19 [00:00<00:00, 22.59it/s][A
 79%|███████▉  | 15/19 [00:00<00:00, 21.01it/s][A
 95%|█████████▍| 18/19 [00:00<00:00, 19.18it/s][A                                                 
                                               [A100%|██████████| 320/320 [01:36<00:00,  6.58it/s]
100%|██████████| 19/19 [00:00<00:00, 19.18it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-320
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-320/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-320/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-320/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-320/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/checkpoint-256 (score: 0.3537117903459564).
                                                 100%|██████████| 320/320 [01:40<00:00,  6.58it/s]100%|██████████| 320/320 [01:40<00:00,  3.18it/s]
Saving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_test.epoch10.bs32/special_tokens_map.json
{'eval_loss': 0.1623125821352005, 'eval_F1_score': 0.33183856497626335, 'eval_Precision': 0.462499999999711, 'eval_Recall': 0.25874125874116827, 'eval_Metric_time': 0.014980554580688477, 'eval_runtime': 0.9784, 'eval_samples_per_second': 613.226, 'eval_steps_per_second': 19.419, 'epoch': 10.0}
{'train_runtime': 100.7766, 'train_samples_per_second': 99.229, 'train_steps_per_second': 3.175, 'train_loss': 0.0768393874168396, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0768
  train_runtime            = 0:01:40.77
  train_samples            =       1000
  train_samples_per_second =     99.229
  train_steps_per_second   =      3.175
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 24.15it/s] 32%|███▏      | 6/19 [00:00<00:00, 21.48it/s] 47%|████▋     | 9/19 [00:00<00:00, 20.98it/s] 63%|██████▎   | 12/19 [00:00<00:00, 21.85it/s] 79%|███████▉  | 15/19 [00:00<00:00, 20.56it/s] 95%|█████████▍| 18/19 [00:00<00:00, 18.97it/s]100%|██████████| 19/19 [00:00<00:00, 19.95it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_F1_score           =     0.2958
  eval_Metric_time        =     0.0148
  eval_Precision          =     0.4012
  eval_Recall             =     0.2343
  eval_loss               =     0.1668
  eval_runtime            = 0:00:01.01
  eval_samples            =        600
  eval_samples_per_second =    592.394
  eval_steps_per_second   =     18.759
  0%|          | 0/35 [00:00<?, ?it/s] 11%|█▏        | 4/35 [00:00<00:00, 35.65it/s] 23%|██▎       | 8/35 [00:00<00:00, 32.24it/s] 34%|███▍      | 12/35 [00:00<00:00, 30.83it/s] 46%|████▌     | 16/35 [00:00<00:00, 25.90it/s] 54%|█████▍    | 19/35 [00:00<00:00, 22.84it/s] 63%|██████▎   | 22/35 [00:00<00:00, 22.34it/s] 71%|███████▏  | 25/35 [00:01<00:00, 21.59it/s] 80%|████████  | 28/35 [00:01<00:00, 21.50it/s] 89%|████████▊ | 31/35 [00:01<00:00, 20.48it/s] 97%|█████████▋| 34/35 [00:01<00:00, 19.23it/s]100%|██████████| 35/35 [00:01<00:00, 21.41it/s]
**********over**********
