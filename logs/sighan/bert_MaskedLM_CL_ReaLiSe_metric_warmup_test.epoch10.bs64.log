Mon Dec 27 07:24:52 UTC 2021
Mon Dec 27 07:24:58 UTC 2021
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM_CL: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM_CL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM_CL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMaskedLM_CL were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['mlp.weight', 'mlp.bias', 'bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp fp16 backend
***** Running training *****
  Num examples = 284201
  Num Epochs = 10
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 44410
Loading Dataset !
Hint: The Data You loading now is the preprocessed sighan from ReaLise, 
Loading Succeed !
  0%|          | 0/44410 [00:00<?, ?it/s]Traceback (most recent call last):
  File "bert_MaskedLM_CL.py", line 319, in <module>
    run()
  File "bert_MaskedLM_CL.py", line 256, in run
    train_result = trainer.train()
  File "/remote-home/xtzhang/anaconda3/envs/115/lib/python3.8/site-packages/transformers/trainer.py", line 1286, in train
    tr_loss += self.training_step(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/115/lib/python3.8/site-packages/transformers/trainer.py", line 1789, in training_step
    self.scaler.scale(loss).backward()
  File "/remote-home/xtzhang/anaconda3/envs/115/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/remote-home/xtzhang/anaconda3/envs/115/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 128, 21128]], which is output 0 of SoftmaxBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
  0%|          | 0/44410 [00:01<?, ?it/s]