Sun Dec 19 05:04:58 UTC 2021
Loading Dataset !
Loading Abs_Pos Bert SigHan Dataset ...
  0%|          | 0/1000 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 190269.64it/s]
  0%|          | 0/600 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:00<00:00, 210205.68it/s]
  0%|          | 0/1100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 469161.52it/s]Sun Dec 19 05:05:11 UTC 2021

Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM_v2: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using amp fp16 backend
***** Running training *****
  Num examples = 1000
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 320
Save cache to cache/sighan_abs_pos_test.
Loading Succeed !
  0%|          | 0/320 [00:00<?, ?it/s]/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  0%|          | 1/320 [00:01<06:07,  1.15s/it]  1%|          | 2/320 [00:01<03:02,  1.74it/s]  1%|          | 3/320 [00:01<02:04,  2.54it/s]  1%|â–         | 4/320 [00:01<01:31,  3.44it/s]  2%|â–         | 5/320 [00:01<01:16,  4.11it/s]  2%|â–         | 6/320 [00:01<01:07,  4.65it/s]  2%|â–         | 7/320 [00:02<01:00,  5.20it/s]  2%|â–Ž         | 8/320 [00:02<01:04,  4.83it/s]  3%|â–Ž         | 9/320 [00:02<01:00,  5.16it/s]  3%|â–Ž         | 10/320 [00:02<00:56,  5.48it/s]  3%|â–Ž         | 11/320 [00:02<00:52,  5.93it/s]  4%|â–         | 12/320 [00:02<00:48,  6.34it/s]  4%|â–         | 13/320 [00:03<00:48,  6.38it/s]  4%|â–         | 14/320 [00:03<00:48,  6.37it/s]  5%|â–         | 15/320 [00:03<00:48,  6.31it/s]  5%|â–Œ         | 16/320 [00:03<00:48,  6.27it/s]  5%|â–Œ         | 17/320 [00:03<00:46,  6.54it/s]  6%|â–Œ         | 18/320 [00:03<00:45,  6.61it/s]  6%|â–Œ         | 19/320 [00:03<00:43,  6.85it/s]  6%|â–‹         | 20/320 [00:04<00:44,  6.77it/s]  7%|â–‹         | 21/320 [00:04<00:46,  6.38it/s]  7%|â–‹         | 22/320 [00:04<00:45,  6.52it/s]  7%|â–‹         | 23/320 [00:04<00:42,  6.92it/s]  8%|â–Š         | 24/320 [00:04<00:45,  6.45it/s]  8%|â–Š         | 25/320 [00:04<00:44,  6.68it/s]  8%|â–Š         | 26/320 [00:05<00:44,  6.61it/s]  8%|â–Š         | 27/320 [00:05<00:48,  6.09it/s]  9%|â–‰         | 28/320 [00:05<00:48,  6.02it/s]  9%|â–‰         | 29/320 [00:05<00:49,  5.92it/s]  9%|â–‰         | 30/320 [00:05<00:50,  5.73it/s] 10%|â–‰         | 31/320 [00:05<00:48,  6.00it/s]
  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.10it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.33it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.33it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 21.47it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.11it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.70it/s][A                                                
                                               [A 10%|â–ˆ         | 32/320 [00:07<00:48,  6.00it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.70it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-32
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-32/special_tokens_map.json
 10%|â–ˆ         | 33/320 [00:10<05:54,  1.24s/it] 11%|â–ˆ         | 34/320 [00:11<04:35,  1.04it/s] 11%|â–ˆ         | 35/320 [00:11<03:38,  1.30it/s] 11%|â–ˆâ–        | 36/320 [00:11<02:49,  1.68it/s] 12%|â–ˆâ–        | 37/320 [00:11<02:14,  2.11it/s] 12%|â–ˆâ–        | 38/320 [00:11<01:47,  2.62it/s] 12%|â–ˆâ–        | 39/320 [00:11<01:30,  3.10it/s] 12%|â–ˆâ–Ž        | 40/320 [00:12<01:17,  3.63it/s] 13%|â–ˆâ–Ž        | 41/320 [00:12<01:07,  4.12it/s] 13%|â–ˆâ–Ž        | 42/320 [00:12<00:59,  4.71it/s] 13%|â–ˆâ–Ž        | 43/320 [00:12<00:57,  4.84it/s] 14%|â–ˆâ–        | 44/320 [00:12<00:52,  5.30it/s] 14%|â–ˆâ–        | 45/320 [00:12<00:49,  5.55it/s] 14%|â–ˆâ–        | 46/320 [00:13<00:50,  5.46it/s] 15%|â–ˆâ–        | 47/320 [00:13<00:46,  5.87it/s] 15%|â–ˆâ–Œ        | 48/320 [00:13<00:43,  6.22it/s] 15%|â–ˆâ–Œ        | 49/320 [00:13<00:42,  6.36it/s] 16%|â–ˆâ–Œ        | 50/320 [00:13<00:40,  6.61it/s] 16%|â–ˆâ–Œ        | 51/320 [00:13<00:40,  6.65it/s] 16%|â–ˆâ–‹        | 52/320 [00:13<00:39,  6.82it/s] 17%|â–ˆâ–‹        | 53/320 [00:14<00:38,  7.03it/s] 17%|â–ˆâ–‹        | 54/320 [00:14<00:41,  6.35it/s] 17%|â–ˆâ–‹        | 55/320 [00:14<00:41,  6.38it/s] 18%|â–ˆâ–Š        | 56/320 [00:14<00:40,  6.52it/s] 18%|â–ˆâ–Š        | 57/320 [00:14<00:41,  6.31it/s] 18%|â–ˆâ–Š        | 58/320 [00:14<00:41,  6.24it/s] 18%|â–ˆâ–Š        | 59/320 [00:15<00:42,  6.08it/s] 19%|â–ˆâ–‰        | 60/320 [00:15<00:42,  6.09it/s] 19%|â–ˆâ–‰        | 61/320 [00:15<00:43,  5.96it/s] 19%|â–ˆâ–‰        | 62/320 [00:15<00:42,  6.10it/s] 20%|â–ˆâ–‰        | 63/320 [00:15<00:41,  6.19it/s] 20%|â–ˆâ–ˆ        | 64/320 [00:15<00:37,  6.92it/s]{'eval_loss': 0.13109168410301208, 'eval_F1_score': 0.1724137930560828, 'eval_Precision': 0.22471910112346927, 'eval_Recall': 0.13986013986009097, 'eval_Metric_time': 0.020085573196411133, 'eval_runtime': 1.017, 'eval_samples_per_second': 589.96, 'eval_steps_per_second': 18.682, 'epoch': 1.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 24.05it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 21.64it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.10it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 21.78it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.15it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.53it/s][A                                                
                                               [A 20%|â–ˆâ–ˆ        | 64/320 [00:16<00:37,  6.92it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.53it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-64
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-64/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-64/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-64/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-64/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 65/320 [00:20<06:31,  1.54s/it] 21%|â–ˆâ–ˆ        | 66/320 [00:20<04:47,  1.13s/it] 21%|â–ˆâ–ˆ        | 67/320 [00:20<03:30,  1.20it/s] 21%|â–ˆâ–ˆâ–       | 68/320 [00:21<02:39,  1.58it/s] 22%|â–ˆâ–ˆâ–       | 69/320 [00:21<02:02,  2.05it/s] 22%|â–ˆâ–ˆâ–       | 70/320 [00:21<01:36,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 71/320 [00:21<01:18,  3.18it/s] 22%|â–ˆâ–ˆâ–Ž       | 72/320 [00:21<01:08,  3.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 73/320 [00:21<00:58,  4.24it/s] 23%|â–ˆâ–ˆâ–Ž       | 74/320 [00:21<00:50,  4.86it/s] 23%|â–ˆâ–ˆâ–Ž       | 75/320 [00:22<00:49,  4.98it/s] 24%|â–ˆâ–ˆâ–       | 76/320 [00:22<00:47,  5.15it/s] 24%|â–ˆâ–ˆâ–       | 77/320 [00:22<00:43,  5.57it/s] 24%|â–ˆâ–ˆâ–       | 78/320 [00:22<00:43,  5.59it/s] 25%|â–ˆâ–ˆâ–       | 79/320 [00:22<00:42,  5.67it/s] 25%|â–ˆâ–ˆâ–Œ       | 80/320 [00:22<00:41,  5.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 81/320 [00:23<00:43,  5.52it/s] 26%|â–ˆâ–ˆâ–Œ       | 82/320 [00:23<00:43,  5.45it/s] 26%|â–ˆâ–ˆâ–Œ       | 83/320 [00:23<00:46,  5.08it/s] 26%|â–ˆâ–ˆâ–‹       | 84/320 [00:23<00:43,  5.41it/s] 27%|â–ˆâ–ˆâ–‹       | 85/320 [00:23<00:41,  5.66it/s] 27%|â–ˆâ–ˆâ–‹       | 86/320 [00:24<00:39,  5.99it/s] 27%|â–ˆâ–ˆâ–‹       | 87/320 [00:24<00:38,  5.98it/s] 28%|â–ˆâ–ˆâ–Š       | 88/320 [00:24<00:38,  6.05it/s] 28%|â–ˆâ–ˆâ–Š       | 89/320 [00:24<00:36,  6.36it/s] 28%|â–ˆâ–ˆâ–Š       | 90/320 [00:24<00:35,  6.39it/s] 28%|â–ˆâ–ˆâ–Š       | 91/320 [00:24<00:34,  6.69it/s] 29%|â–ˆâ–ˆâ–‰       | 92/320 [00:24<00:36,  6.29it/s] 29%|â–ˆâ–ˆâ–‰       | 93/320 [00:25<00:35,  6.47it/s] 29%|â–ˆâ–ˆâ–‰       | 94/320 [00:25<00:35,  6.43it/s] 30%|â–ˆâ–ˆâ–‰       | 95/320 [00:25<00:35,  6.40it/s]{'eval_loss': 0.11963073909282684, 'eval_F1_score': 0.26180257505977084, 'eval_Precision': 0.33888888888870067, 'eval_Recall': 0.21328671328663873, 'eval_Metric_time': 0.01519465446472168, 'eval_runtime': 1.0234, 'eval_samples_per_second': 586.262, 'eval_steps_per_second': 18.565, 'epoch': 2.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.94it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.70it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.10it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 21.54it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.31it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.79it/s][A                                                
                                               [A 30%|â–ˆâ–ˆâ–ˆ       | 96/320 [00:26<00:34,  6.40it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.79it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-96
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-96/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-96/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-96/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-96/special_tokens_map.json
 30%|â–ˆâ–ˆâ–ˆ       | 97/320 [00:30<04:27,  1.20s/it] 31%|â–ˆâ–ˆâ–ˆ       | 98/320 [00:30<03:29,  1.06it/s] 31%|â–ˆâ–ˆâ–ˆ       | 99/320 [00:30<02:42,  1.36it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 100/320 [00:30<02:06,  1.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 101/320 [00:30<01:39,  2.20it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 102/320 [00:31<01:18,  2.79it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 103/320 [00:31<01:05,  3.29it/s] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 104/320 [00:31<00:55,  3.87it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 105/320 [00:31<00:50,  4.29it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 106/320 [00:31<00:44,  4.78it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 107/320 [00:31<00:42,  4.99it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 108/320 [00:31<00:39,  5.38it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 109/320 [00:32<00:36,  5.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 110/320 [00:32<00:34,  6.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 111/320 [00:32<00:34,  6.07it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 112/320 [00:32<00:36,  5.75it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 113/320 [00:32<00:35,  5.85it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 114/320 [00:32<00:34,  5.97it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 115/320 [00:33<00:33,  6.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 116/320 [00:33<00:32,  6.32it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 117/320 [00:33<00:32,  6.28it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 118/320 [00:33<00:30,  6.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 119/320 [00:33<00:31,  6.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 120/320 [00:33<00:30,  6.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 121/320 [00:34<00:32,  6.20it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 122/320 [00:34<00:30,  6.42it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 123/320 [00:34<00:30,  6.36it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 124/320 [00:34<00:31,  6.13it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 125/320 [00:34<00:34,  5.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 126/320 [00:34<00:33,  5.82it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 127/320 [00:35<00:33,  5.72it/s]{'eval_loss': 0.12311173975467682, 'eval_F1_score': 0.3168724279349744, 'eval_Precision': 0.3849999999998075, 'eval_Recall': 0.2692307692306751, 'eval_Metric_time': 0.015482187271118164, 'eval_runtime': 1.0055, 'eval_samples_per_second': 596.718, 'eval_steps_per_second': 18.896, 'epoch': 3.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.84it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.81it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.80it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 22.20it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.69it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.92it/s][A                                                 
                                               [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 128/320 [00:36<00:33,  5.72it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.92it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-128
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-128/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-128/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-128/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-128/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 129/320 [00:39<03:46,  1.19s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 130/320 [00:39<02:58,  1.06it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 131/320 [00:40<02:19,  1.35it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 132/320 [00:40<01:48,  1.74it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 133/320 [00:40<01:25,  2.19it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 134/320 [00:40<01:08,  2.70it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 135/320 [00:40<00:56,  3.25it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 136/320 [00:40<00:50,  3.67it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 137/320 [00:41<00:41,  4.38it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 138/320 [00:41<00:36,  4.96it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 139/320 [00:41<00:34,  5.32it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 140/320 [00:41<00:31,  5.72it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 141/320 [00:41<00:30,  5.91it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 142/320 [00:41<00:30,  5.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 143/320 [00:41<00:29,  6.03it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 144/320 [00:42<00:29,  5.97it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 145/320 [00:42<00:32,  5.36it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 146/320 [00:42<00:30,  5.68it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 147/320 [00:42<00:29,  5.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 148/320 [00:42<00:29,  5.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 149/320 [00:43<00:27,  6.32it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 150/320 [00:43<00:28,  5.89it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 151/320 [00:43<00:29,  5.77it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 152/320 [00:43<00:28,  5.81it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 153/320 [00:43<00:27,  6.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 154/320 [00:43<00:27,  6.14it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 155/320 [00:44<00:26,  6.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 156/320 [00:44<00:26,  6.22it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 157/320 [00:44<00:26,  6.14it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 158/320 [00:44<00:24,  6.50it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 159/320 [00:44<00:25,  6.35it/s]{'eval_loss': 0.13928410410881042, 'eval_F1_score': 0.3217391303876068, 'eval_Precision': 0.4252873563215947, 'eval_Recall': 0.25874125874116827, 'eval_Metric_time': 0.0166318416595459, 'eval_runtime': 0.9965, 'eval_samples_per_second': 602.085, 'eval_steps_per_second': 19.066, 'epoch': 4.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.85it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.71it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.73it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 22.22it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.61it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.83it/s][A                                                 
                                               [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 160/320 [00:45<00:25,  6.35it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.83it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-160
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-160/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-160/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-160/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-160/special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 161/320 [00:49<03:08,  1.18s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 162/320 [00:49<02:27,  1.07it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 163/320 [00:49<01:54,  1.37it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/320 [00:49<01:29,  1.75it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 165/320 [00:50<01:13,  2.10it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 166/320 [00:50<00:58,  2.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 167/320 [00:50<00:48,  3.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 168/320 [00:50<00:40,  3.79it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 169/320 [00:50<00:35,  4.26it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 170/320 [00:50<00:30,  4.88it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 171/320 [00:51<00:29,  5.01it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 172/320 [00:51<00:27,  5.29it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 173/320 [00:51<00:26,  5.61it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 174/320 [00:51<00:24,  5.88it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 175/320 [00:51<00:23,  6.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 176/320 [00:51<00:22,  6.36it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 177/320 [00:51<00:23,  6.08it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 178/320 [00:52<00:23,  6.08it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 179/320 [00:52<00:21,  6.56it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 180/320 [00:52<00:21,  6.45it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 181/320 [00:52<00:21,  6.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 182/320 [00:52<00:20,  6.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 183/320 [00:52<00:20,  6.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 184/320 [00:53<00:19,  6.80it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 185/320 [00:53<00:21,  6.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 186/320 [00:53<00:22,  5.93it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 187/320 [00:53<00:21,  6.21it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 188/320 [00:53<00:20,  6.36it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 189/320 [00:53<00:21,  6.14it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 190/320 [00:54<00:22,  5.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 191/320 [00:54<00:22,  5.71it/s]{'eval_loss': 0.14125394821166992, 'eval_F1_score': 0.33403805492033667, 'eval_Precision': 0.42245989304790244, 'eval_Recall': 0.27622377622367966, 'eval_Metric_time': 0.015707015991210938, 'eval_runtime': 0.997, 'eval_samples_per_second': 601.82, 'eval_steps_per_second': 19.058, 'epoch': 5.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 24.64it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 20.48it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 19.90it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 20.68it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 19.48it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:00<00:00, 17.64it/s][A                                                 
                                               [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 192/320 [00:55<00:22,  5.71it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 17.64it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-192
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-192/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-192/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-192/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-192/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 193/320 [00:59<02:35,  1.22s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 194/320 [00:59<02:01,  1.04it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 195/320 [00:59<01:35,  1.32it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 196/320 [00:59<01:13,  1.68it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 197/320 [00:59<00:57,  2.13it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 198/320 [00:59<00:46,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 199/320 [01:00<00:38,  3.16it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 200/320 [01:00<00:32,  3.68it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 201/320 [01:00<00:30,  3.88it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 202/320 [01:00<00:27,  4.36it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 203/320 [01:00<00:25,  4.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 204/320 [01:00<00:23,  4.96it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 205/320 [01:01<00:21,  5.31it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 206/320 [01:01<00:21,  5.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 207/320 [01:01<00:19,  5.77it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 208/320 [01:01<00:18,  6.09it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 209/320 [01:01<00:17,  6.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 210/320 [01:01<00:17,  6.14it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 211/320 [01:02<00:17,  6.16it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 212/320 [01:02<00:17,  6.29it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 213/320 [01:02<00:17,  6.01it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 214/320 [01:02<00:17,  6.21it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 215/320 [01:02<00:16,  6.28it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 216/320 [01:02<00:17,  6.03it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 217/320 [01:03<00:16,  6.32it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 218/320 [01:03<00:16,  6.20it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 219/320 [01:03<00:16,  6.30it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 220/320 [01:03<00:15,  6.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 221/320 [01:03<00:14,  6.70it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 222/320 [01:03<00:14,  6.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 223/320 [01:04<00:15,  6.20it/s]{'eval_loss': 0.1382492333650589, 'eval_F1_score': 0.2881002087200317, 'eval_Precision': 0.35751295336769046, 'eval_Recall': 0.2412587412586569, 'eval_Metric_time': 0.031900644302368164, 'eval_runtime': 1.0752, 'eval_samples_per_second': 558.031, 'eval_steps_per_second': 17.671, 'epoch': 6.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 26.02it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 21.48it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 20.68it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 21.10it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.04it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.55it/s][A                                                 
                                               [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 224/320 [01:05<00:15,  6.20it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.55it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-224
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-224/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-224/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-224/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-224/special_tokens_map.json
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 225/320 [01:08<01:54,  1.21s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 226/320 [01:09<01:28,  1.06it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 227/320 [01:09<01:09,  1.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 228/320 [01:09<00:53,  1.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 229/320 [01:09<00:42,  2.15it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 230/320 [01:09<00:33,  2.70it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 231/320 [01:09<00:27,  3.20it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 232/320 [01:09<00:23,  3.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 233/320 [01:10<00:20,  4.27it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 234/320 [01:10<00:17,  4.86it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 235/320 [01:10<00:15,  5.33it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 236/320 [01:10<00:14,  5.65it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 237/320 [01:10<00:13,  6.02it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 238/320 [01:10<00:13,  6.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 239/320 [01:11<00:13,  5.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 240/320 [01:11<00:13,  5.83it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 241/320 [01:11<00:13,  5.65it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 242/320 [01:11<00:12,  6.13it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 243/320 [01:11<00:12,  6.01it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 244/320 [01:11<00:13,  5.84it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 245/320 [01:12<00:12,  6.08it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 246/320 [01:12<00:11,  6.46it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 247/320 [01:12<00:12,  6.01it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 248/320 [01:12<00:13,  5.50it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 249/320 [01:12<00:12,  5.78it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 250/320 [01:12<00:11,  6.08it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 251/320 [01:13<00:11,  5.79it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 252/320 [01:13<00:10,  6.33it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 253/320 [01:13<00:09,  6.78it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 254/320 [01:13<00:10,  6.36it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 255/320 [01:13<00:10,  6.41it/s]{'eval_loss': 0.15065130591392517, 'eval_F1_score': 0.3383297644063442, 'eval_Precision': 0.43646408839754897, 'eval_Recall': 0.27622377622367966, 'eval_Metric_time': 0.015431642532348633, 'eval_runtime': 1.0224, 'eval_samples_per_second': 586.874, 'eval_steps_per_second': 18.584, 'epoch': 7.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.73it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 21.93it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 20.51it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 20.75it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 19.35it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:00<00:00, 17.77it/s][A                                                 
                                               [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 256/320 [01:14<00:09,  6.41it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 17.77it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 257/320 [01:18<01:13,  1.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 258/320 [01:18<00:56,  1.10it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 259/320 [01:18<00:43,  1.41it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 260/320 [01:18<00:33,  1.80it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 261/320 [01:18<00:26,  2.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 262/320 [01:19<00:20,  2.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 263/320 [01:19<00:17,  3.35it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 264/320 [01:19<00:14,  3.89it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 265/320 [01:19<00:12,  4.34it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 266/320 [01:19<00:11,  4.89it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 267/320 [01:19<00:10,  5.27it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 268/320 [01:19<00:09,  5.50it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 269/320 [01:20<00:09,  5.58it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 270/320 [01:20<00:08,  5.65it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 271/320 [01:20<00:08,  5.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 272/320 [01:20<00:08,  5.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 273/320 [01:20<00:08,  5.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 274/320 [01:21<00:08,  5.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 275/320 [01:21<00:07,  5.76it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 276/320 [01:21<00:07,  5.93it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 277/320 [01:21<00:07,  5.38it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 278/320 [01:21<00:07,  5.35it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 279/320 [01:21<00:07,  5.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 280/320 [01:22<00:06,  6.08it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 281/320 [01:22<00:06,  6.21it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 282/320 [01:22<00:06,  6.30it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 283/320 [01:22<00:05,  6.69it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 284/320 [01:22<00:05,  6.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 285/320 [01:22<00:05,  6.68it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 286/320 [01:22<00:05,  6.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 287/320 [01:23<00:04,  6.63it/s]{'eval_loss': 0.16612432897090912, 'eval_F1_score': 0.3537117903459564, 'eval_Precision': 0.47093023255786576, 'eval_Recall': 0.2832167832166842, 'eval_Metric_time': 0.015639305114746094, 'eval_runtime': 1.0396, 'eval_samples_per_second': 577.151, 'eval_steps_per_second': 18.276, 'epoch': 8.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 25.42it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.44it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.80it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 22.19it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.70it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.87it/s][A                                                 
                                               [A 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 288/320 [01:24<00:04,  6.63it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.87it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-288
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-288/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-288/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-288/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-288/special_tokens_map.json
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 289/320 [01:28<00:37,  1.21s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 290/320 [01:28<00:28,  1.04it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 291/320 [01:28<00:21,  1.34it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 292/320 [01:28<00:16,  1.68it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 293/320 [01:28<00:12,  2.13it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 294/320 [01:28<00:10,  2.50it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 295/320 [01:29<00:08,  3.07it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 296/320 [01:29<00:06,  3.59it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 297/320 [01:29<00:05,  4.06it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 298/320 [01:29<00:04,  4.54it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 299/320 [01:29<00:04,  4.94it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 300/320 [01:29<00:03,  5.45it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 301/320 [01:30<00:03,  5.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 302/320 [01:30<00:03,  5.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 303/320 [01:30<00:02,  6.25it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 304/320 [01:30<00:02,  6.19it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 305/320 [01:30<00:02,  6.30it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 306/320 [01:30<00:02,  6.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 307/320 [01:30<00:01,  6.51it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 308/320 [01:31<00:01,  6.77it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 309/320 [01:31<00:01,  6.81it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 310/320 [01:31<00:01,  6.81it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 311/320 [01:31<00:01,  6.53it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 312/320 [01:31<00:01,  6.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 313/320 [01:31<00:01,  6.68it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 314/320 [01:31<00:00,  6.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 315/320 [01:32<00:00,  6.31it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 316/320 [01:32<00:00,  6.32it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 317/320 [01:32<00:00,  6.34it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 318/320 [01:32<00:00,  5.97it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 319/320 [01:32<00:00,  6.14it/s]{'eval_loss': 0.16420115530490875, 'eval_F1_score': 0.3199999999535328, 'eval_Precision': 0.43902439024363477, 'eval_Recall': 0.25174825174816373, 'eval_Metric_time': 0.015186309814453125, 'eval_runtime': 0.9998, 'eval_samples_per_second': 600.094, 'eval_steps_per_second': 19.003, 'epoch': 9.0}

  0%|          | 0/19 [00:00<?, ?it/s][A
 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 24.93it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 22.52it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.71it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 21.17it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 19.81it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.29it/s][A                                                 
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [01:33<00:00,  6.14it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 18.29it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-320
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-320/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-320/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-320/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-320/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/checkpoint-256 (score: 0.3537117903459564).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [01:38<00:00,  6.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [01:38<00:00,  3.25it/s]
Saving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_mask_hiddendropout_expand.epoch10.bs32/special_tokens_map.json
{'eval_loss': 0.1623125821352005, 'eval_F1_score': 0.33183856497626335, 'eval_Precision': 0.462499999999711, 'eval_Recall': 0.25874125874116827, 'eval_Metric_time': 0.018086910247802734, 'eval_runtime': 1.0305, 'eval_samples_per_second': 582.239, 'eval_steps_per_second': 18.438, 'epoch': 10.0}
{'train_runtime': 98.3226, 'train_samples_per_second': 101.706, 'train_steps_per_second': 3.255, 'train_loss': 0.0768393874168396, 'epoch': 10.0}
***** train metrics *****
  epoch                    =       10.0
  train_loss               =     0.0768
  train_runtime            = 0:01:38.32
  train_samples            =       1000
  train_samples_per_second =    101.706
  train_steps_per_second   =      3.255
  0%|          | 0/19 [00:00<?, ?it/s] 16%|â–ˆâ–Œ        | 3/19 [00:00<00:00, 23.80it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:00, 21.72it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 21.42it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:00<00:00, 22.02it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:00<00:00, 20.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:00<00:00, 18.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 19.88it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_F1_score           =     0.2958
  eval_Metric_time        =     0.0158
  eval_Precision          =     0.4012
  eval_Recall             =     0.2343
  eval_loss               =     0.1668
  eval_runtime            = 0:00:01.01
  eval_samples            =        600
  eval_samples_per_second =    590.695
  eval_steps_per_second   =     18.705
  0%|          | 0/35 [00:00<?, ?it/s] 11%|â–ˆâ–        | 4/35 [00:00<00:00, 37.88it/s] 23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 32.61it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 30.22it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 25.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 22.45it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:00<00:00, 21.98it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:01<00:00, 21.24it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 21.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 20.20it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 19.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 21.08it/s]
**********over**********
