Thu Dec 23 07:45:57 UTC 2021
Thu Dec 23 07:46:04 UTC 2021
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using amp fp16 backend
***** Running training *****
  Num examples = 2000
  Num Epochs = 1
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 16
Loading Dataset !
Hint: The Data You loading now is the preprocessed sighan from ReaLise, 
Loading Succeed !
  0%|          | 0/16 [00:00<?, ?it/s]  6%|â–‹         | 1/16 [00:02<00:31,  2.09s/it] 12%|â–ˆâ–Ž        | 2/16 [00:02<00:14,  1.04s/it] 19%|â–ˆâ–‰        | 3/16 [00:02<00:09,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:03<00:06,  1.76it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:03<00:05,  2.06it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:04,  2.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:04<00:03,  2.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:04<00:03,  2.62it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:02,  2.72it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:05<00:02,  2.80it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:01,  2.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:05<00:01,  2.99it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  2.98it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:06<00:00,  2.97it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:06<00:00,  2.97it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:06<00:00,  3.30it/s]
  0%|          | 0/9 [00:00<?, ?it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 19.86it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:00<00:00, 11.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:00<00:00, 10.56it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.79it/s][A                                               
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  3.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.79it/s][A
                                             [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16
Configuration saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/checkpoint-16 (score: 0.0).
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:13<00:00,  3.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:13<00:00,  1.20it/s]
Saving model checkpoint to ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128
Configuration saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_base_ReaLiSe_test_metric.epoch1.bs128/special_tokens_map.json
{'eval_loss': 0.3490868806838989, 'eval_F1_score': 0.0, 'eval_Precision': 0.0, 'eval_Recall': 0.0, 'eval_Metric_time': 0.026266098022460938, 'eval_runtime': 0.9486, 'eval_samples_per_second': 1159.636, 'eval_steps_per_second': 9.488, 'epoch': 1.0}
{'train_runtime': 13.3701, 'train_samples_per_second': 149.588, 'train_steps_per_second': 1.197, 'train_loss': 0.46430081129074097, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     0.4643
  train_runtime            = 0:00:13.37
  train_samples            =       2000
  train_samples_per_second =    149.588
  train_steps_per_second   =      1.197
  0%|          | 0/9 [00:00<?, ?it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 19.54it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:00<00:00, 11.53it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:00<00:00, 10.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.29it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_F1_score           =        0.0
  eval_Metric_time        =     0.0264
  eval_Precision          =        0.0
  eval_Recall             =        0.0
  eval_loss               =     0.3491
  eval_runtime            = 0:00:00.95
  eval_samples            =       1100
  eval_samples_per_second =   1148.498
  eval_steps_per_second   =      9.397
  0%|          | 0/9 [00:00<?, ?it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:00<00:00, 20.05it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:00<00:00, 11.08it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:00<00:00,  9.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.67it/s]
**********over**********
