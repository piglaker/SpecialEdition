Mon Jan 17 07:22:57 UTC 2022
Mon Jan 17 07:23:05 UTC 2022
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing ProtoBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing ProtoBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ProtoBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ProtoBertForMaskedLM were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['bias', 'mlp.weight', 'mlp.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp fp16 backend
***** Running training *****
  Num examples = 284201
  Num Epochs = 10
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 22210
Loading Dataset !
Hint: The Data You loading now is the preprocessed sighan from ReaLise, 
Hint: Using **SIGHAN15** for eval & test !
Loading Succeed !
Hint: Loading Model *****Proto*****
  0%|          | 0/22210 [00:00<?, ?it/s]Traceback (most recent call last):
  File "proto_model.py", line 181, in <module>
    run()
  File "proto_model.py", line 118, in run
    train_result = trainer.train()
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1289, in train
    tr_loss += self.training_step(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1780, in training_step
    loss = self.compute_loss(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/transformers/trainer.py", line 1814, in compute_loss
    outputs = model(**inputs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/models/bert/modeling_bert_v4.py", line 1517, in forward
    distill_scores = self.cls(distill_hiddens)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/models/bert/modeling_bert_v4.py", line 683, in forward
    prediction_scores = self.predictions(sequence_output)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/models/bert/modeling_bert_v4.py", line 673, in forward
    hidden_states = self.decoder(hidden_states)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/remote-home/xtzhang/anaconda3/envs/117/lib/python3.8/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 662.00 MiB (GPU 0; 23.70 GiB total capacity; 21.81 GiB already allocated; 304.56 MiB free; 21.95 GiB reserved in total by PyTorch)
  0%|          | 0/22210 [00:06<?, ?it/s]