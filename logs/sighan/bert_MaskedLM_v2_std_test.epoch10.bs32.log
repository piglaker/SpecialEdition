Tue Dec 14 06:30:30 UTC 2021
Loading Dataset !
Loading Abs_Pos Bert SigHan Dataset ...
  0%|          | 0/1000 [00:00<?, ?it/s]100%|██████████| 1000/1000 [00:00<00:00, 58901.32it/s]
  0%|          | 0/600 [00:00<?, ?it/s]100%|██████████| 600/600 [00:00<00:00, 73056.65it/s]
  0%|          | 0/1100 [00:00<?, ?it/s]100%|██████████| 1100/1100 [00:00<00:00, 56143.87it/s]Tue Dec 14 06:30:44 UTC 2021

Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM_v2: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM_v2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using amp fp16 backend
***** Running training *****
  Num examples = 1000
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 320
Save cache to cache/sighan_abs_pos_test.
Loading Succeed !
  0%|          | 0/320 [00:00<?, ?it/s]/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py:1317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
  0%|          | 1/320 [00:00<03:09,  1.68it/s]  1%|          | 2/320 [00:01<03:18,  1.60it/s]  1%|          | 3/320 [00:01<03:27,  1.53it/s]  1%|▏         | 4/320 [00:02<03:20,  1.58it/s]  2%|▏         | 5/320 [00:03<03:14,  1.62it/s]  2%|▏         | 6/320 [00:03<03:14,  1.62it/s]  2%|▏         | 7/320 [00:04<03:06,  1.68it/s]  2%|▎         | 8/320 [00:05<03:37,  1.43it/s]  3%|▎         | 9/320 [00:05<03:34,  1.45it/s]  3%|▎         | 10/320 [00:06<03:29,  1.48it/s]  3%|▎         | 11/320 [00:07<03:20,  1.54it/s]  4%|▍         | 12/320 [00:07<03:10,  1.62it/s]  4%|▍         | 13/320 [00:08<03:09,  1.62it/s]  4%|▍         | 14/320 [00:08<03:11,  1.60it/s]  5%|▍         | 15/320 [00:09<03:15,  1.56it/s]  5%|▌         | 16/320 [00:10<03:14,  1.56it/s]  5%|▌         | 17/320 [00:10<03:05,  1.64it/s]  6%|▌         | 18/320 [00:11<03:02,  1.65it/s]  6%|▌         | 19/320 [00:11<02:55,  1.72it/s]  6%|▋         | 20/320 [00:12<02:59,  1.67it/s]  7%|▋         | 21/320 [00:13<03:10,  1.57it/s]  7%|▋         | 22/320 [00:13<03:07,  1.59it/s]  7%|▋         | 23/320 [00:14<02:57,  1.67it/s]  8%|▊         | 24/320 [00:15<03:13,  1.53it/s]  8%|▊         | 25/320 [00:15<03:01,  1.63it/s]  8%|▊         | 26/320 [00:16<03:03,  1.60it/s]  8%|▊         | 27/320 [00:17<03:19,  1.47it/s]  9%|▉         | 28/320 [00:17<03:19,  1.47it/s]  9%|▉         | 29/320 [00:18<03:20,  1.45it/s]  9%|▉         | 30/320 [00:19<03:27,  1.40it/s] 10%|▉         | 31/320 [00:19<03:19,  1.45it/s] 10%|█         | 32/320 [00:20<02:36,  1.84it/s]
  0%|          | 0/19 [00:00<?, ?it/s][A
 11%|█         | 2/19 [00:00<00:01, 12.17it/s][A
 21%|██        | 4/19 [00:00<00:01,  7.85it/s][A
 26%|██▋       | 5/19 [00:00<00:01,  7.22it/s][A
 32%|███▏      | 6/19 [00:00<00:01,  7.32it/s][A
 37%|███▋      | 7/19 [00:00<00:01,  7.45it/s][A
 42%|████▏     | 8/19 [00:01<00:01,  6.88it/s][A
 47%|████▋     | 9/19 [00:01<00:01,  7.16it/s][A
 53%|█████▎    | 10/19 [00:01<00:01,  7.44it/s][A
 58%|█████▊    | 11/19 [00:01<00:01,  7.47it/s][A
 63%|██████▎   | 12/19 [00:01<00:00,  7.65it/s][A
 68%|██████▊   | 13/19 [00:01<00:00,  7.03it/s][A
 74%|███████▎  | 14/19 [00:01<00:00,  7.42it/s][A
 79%|███████▉  | 15/19 [00:02<00:00,  6.89it/s][A
 84%|████████▍ | 16/19 [00:02<00:00,  5.59it/s][A
 89%|████████▉ | 17/19 [00:02<00:00,  5.87it/s][A
 95%|█████████▍| 18/19 [00:02<00:00,  6.31it/s][A
100%|██████████| 19/19 [00:02<00:00,  6.62it/s][A                                                
                                               [A 10%|█         | 32/320 [00:23<02:36,  1.84it/s]
100%|██████████| 19/19 [00:02<00:00,  6.62it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_v2_std_test.epoch10.bs32/checkpoint-32
Configuration saved in ./tmp/sighan/bert_MaskedLM_v2_std_test.epoch10.bs32/checkpoint-32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_v2_std_test.epoch10.bs32/checkpoint-32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_v2_std_test.epoch10.bs32/checkpoint-32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_v2_std_test.epoch10.bs32/checkpoint-32/special_tokens_map.json
 10%|█         | 33/320 [00:37<26:14,  5.48s/it] 11%|█         | 34/320 [00:37<19:02,  3.99s/it]{'eval_loss': 0.25043588876724243, 'eval_F1_score': 0.01388888884459662, 'eval_Precision': 0.010380622837368446, 'eval_Recall': 0.020979020979013645, 'eval_Metric_time': 0.03246808052062988, 'eval_runtime': 2.8964, 'eval_samples_per_second': 207.152, 'eval_steps_per_second': 6.56, 'epoch': 1.0}
Traceback (most recent call last):
  File "bert_MaskedLM_v2.py", line 336, in <module>
    run()
  File "bert_MaskedLM_v2.py", line 273, in run
    train_result = trainer.train()
  File "/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py", line 1289, in train
    tr_loss += self.training_step(model, inputs)
  File "/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/transformers/trainer.py", line 1793, in training_step
    self.scaler.scale(loss).backward()
  File "/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/remote-home/xtzhang/anaconda3/envs/dophin/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 620.00 MiB (GPU 0; 10.92 GiB total capacity; 9.09 GiB already allocated; 139.38 MiB free; 10.01 GiB reserved in total by PyTorch)
 11%|█         | 34/320 [00:38<05:22,  1.13s/it]