Tue Dec 21 08:31:29 UTC 2021
Loading Dataset !
Loading SigHan Dataset ...
  0%|          | 0/2000 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 97565.78it/s]
  0%|          | 0/1100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 165854.28it/s]
  0%|          | 0/1100 [00:00<?, ?it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 396/1100 [00:00<00:00, 3088.27it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 8310.97it/s]Tue Dec 21 08:31:39 UTC 2021

Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM_CL: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM_CL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM_CL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Using amp fp16 backend
***** Running training *****
  Num examples = 2000
  Num Epochs = 1
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 63
Loading Succeed !
  0%|          | 0/63 [00:00<?, ?it/s]  2%|â–         | 1/63 [00:00<00:14,  4.13it/s]  3%|â–Ž         | 2/63 [00:00<00:10,  5.81it/s]  5%|â–         | 3/63 [00:00<00:08,  6.85it/s]  6%|â–‹         | 4/63 [00:00<00:07,  7.47it/s]  8%|â–Š         | 5/63 [00:00<00:07,  7.89it/s] 10%|â–‰         | 6/63 [00:00<00:06,  8.33it/s] 11%|â–ˆ         | 7/63 [00:00<00:06,  8.63it/s] 13%|â–ˆâ–Ž        | 8/63 [00:01<00:06,  8.76it/s] 14%|â–ˆâ–        | 9/63 [00:01<00:06,  8.94it/s] 16%|â–ˆâ–Œ        | 10/63 [00:01<00:05,  9.11it/s] 17%|â–ˆâ–‹        | 11/63 [00:01<00:05,  9.25it/s] 19%|â–ˆâ–‰        | 12/63 [00:01<00:05,  8.56it/s] 21%|â–ˆâ–ˆ        | 13/63 [00:01<00:06,  8.29it/s] 22%|â–ˆâ–ˆâ–       | 14/63 [00:01<00:05,  8.32it/s] 24%|â–ˆâ–ˆâ–       | 15/63 [00:01<00:05,  8.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:01<00:05,  8.21it/s] 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:02<00:05,  8.29it/s] 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:02<00:05,  8.39it/s] 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [00:02<00:05,  8.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [00:02<00:04,  8.67it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/63 [00:02<00:04,  8.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [00:02<00:04,  8.34it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [00:02<00:04,  8.40it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [00:02<00:04,  8.28it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [00:03<00:04,  8.29it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [00:03<00:04,  8.36it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 27/63 [00:03<00:04,  8.16it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [00:03<00:04,  8.10it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [00:03<00:04,  8.15it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [00:03<00:04,  8.03it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [00:03<00:04,  7.96it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [00:03<00:03,  7.85it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [00:04<00:03,  7.86it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [00:04<00:03,  8.07it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [00:04<00:03,  8.10it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [00:04<00:03,  7.96it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [00:04<00:03,  7.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [00:04<00:03,  8.00it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [00:04<00:02,  8.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/63 [00:04<00:02,  8.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [00:05<00:02,  8.09it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [00:05<00:02,  8.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [00:05<00:02,  8.04it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [00:05<00:02,  8.06it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [00:05<00:02,  7.94it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 46/63 [00:05<00:02,  7.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [00:05<00:02,  7.80it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [00:05<00:01,  7.82it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [00:06<00:01,  7.93it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [00:06<00:01,  8.06it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [00:06<00:01,  7.97it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 52/63 [00:06<00:01,  7.94it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [00:06<00:01,  8.00it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [00:06<00:01,  8.11it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [00:06<00:00,  8.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [00:06<00:00,  7.94it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [00:07<00:00,  7.93it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [00:07<00:00,  7.90it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 59/63 [00:07<00:00,  7.95it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [00:07<00:00,  8.01it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [00:07<00:00,  8.06it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [00:07<00:00,  8.15it/s]
  0%|          | 0/35 [00:00<?, ?it/s][A
 14%|â–ˆâ–        | 5/35 [00:00<00:00, 41.55it/s][A
 29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:00, 36.91it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 35.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 26.43it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 24.74it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 25.06it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 25.74it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 24.34it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 23.40it/s][A                                               
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:09<00:00,  8.15it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 23.40it/s][A
                                               [ASaving model checkpoint to ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63
Configuration saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/checkpoint-63 (score: 0.0).
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:14<00:00,  8.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:14<00:00,  4.38it/s]
Saving model checkpoint to ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32
Configuration saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/config.json
Model weights saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/pytorch_model.bin
tokenizer config file saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/tokenizer_config.json
Special tokens file saved in ./tmp/sighan/bert_MaskedLM_CL_raw_test.epoch1.bs32/special_tokens_map.json
{'eval_loss': -3820.964599609375, 'eval_F1_score': 0.0, 'eval_Precision': 0.0, 'eval_Recall': 0.0, 'eval_Metric_time': 0.0333094596862793, 'eval_runtime': 1.3834, 'eval_samples_per_second': 795.171, 'eval_steps_per_second': 25.301, 'epoch': 1.0}
{'train_runtime': 14.3922, 'train_samples_per_second': 138.964, 'train_steps_per_second': 4.377, 'train_loss': -3249.2445436507937, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               = -3249.2445
  train_runtime            = 0:00:14.39
  train_samples            =       2000
  train_samples_per_second =    138.964
  train_steps_per_second   =      4.377
  0%|          | 0/35 [00:00<?, ?it/s] 14%|â–ˆâ–        | 5/35 [00:00<00:00, 39.37it/s] 26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 35.67it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:00<00:00, 34.67it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:00<00:00, 26.96it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 24.97it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:00<00:00, 25.12it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:00<00:00, 25.39it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:01<00:00, 24.07it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:01<00:00, 23.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 24.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 25.72it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_F1_score           =        0.0
  eval_Metric_time        =     0.0285
  eval_Precision          =        0.0
  eval_Recall             =        0.0
  eval_loss               = -3820.9646
  eval_runtime            = 0:00:01.39
  eval_samples            =       1100
  eval_samples_per_second =    786.639
  eval_steps_per_second   =     25.029
  0%|          | 0/35 [00:00<?, ?it/s] 14%|â–ˆâ–        | 5/35 [00:00<00:00, 43.12it/s] 29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:00, 37.29it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 35.65it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 26.68it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:00<00:00, 25.03it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 25.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 26.15it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:01<00:00, 24.67it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 23.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 25.37it/s]
**********over**********
