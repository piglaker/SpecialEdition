

# my version

现有纠错模型，实际应用中效果不好，缺乏泛化性。我们发现所有现有模型都在引入字形拼音建模能力来提高混淆字分辨能力进而提升纠错性能，一个疑问就是：没有人验证pretrained model里面是否已经有了这种信息，那我们来验证下；二是我们验证下现有纠错模型是不是真的靠这种信息获得了很好的纠错能力

1.MLP Probe实验验证了语言模型是能从预训练任务中学到哪两个字之间视觉/读音相似的

2.我们提了一个模型是否会根据原始错误改变输出分布的一个指标，来描述模型的原始错误字利用率，我们先计算了原始数据集上训练的模型的指标，但发现都趋近于100%，我们发现是由于train-test存在非常严重的中correct a->b pair对的overlap问题。

    (1)Dark Correction实验是mask掉原错误字，来让模型仅学到target分布来预测的实验，
        a)我们发现Dark Correction实验下的模型依然很较高的cccr：0.6左右（这是不可能的）,这意味着面对测试集的大部分样本，模型即使在训练时没有见过原始错误字，也能有很大的概率将gold推出（这是由于输出分布的改变），我们的结论是overlap显著地影响了预测的分布，
        b)MLM降低了15%，意味着testset中15%的样本不依赖原始错误仅靠上下文就被猜出，

    (2)Isolation Probe实验是为了避免数据集中overlap的影响，去掉了所有重合的pair对，来研究模型的外推性
        结果：
            a.发现没有显示引入的预训练利用率低，说明显式引入还是必要的，
            b.cccr高的模型出现了反而纠错性能低的情况，说明有的模型虽然能把相似字挑出来，但分辨这些混淆字的能力并没有很强


