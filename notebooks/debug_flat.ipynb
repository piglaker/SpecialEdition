{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition\")\n",
    "\n",
    "from core import get_dataset, get_metrics, argument_init, get_lattice_dataset\n",
    "from lib import subTrainer\n",
    "from data.DatasetLoadingHelper import load_ctc2021, load_sighan\n",
    "#from models.bart.modeling_bart_v2 import BartForConditionalGeneration\n",
    "from models.bert.modeling_bert_v2 import BertForFlat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional,Dict, Union, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import trainer_utils, training_args\n",
    "from transformers.trainer_pt_utils import nested_detach\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from core import get_dataset, get_metrics, argument_init\n",
    "from lib import subTrainer  \n",
    "from data.DatasetLoadingHelper import load_ctc2021, load_sighan\n",
    "#from models.bart.modeling_bart_v2 import BartForConditionalGeneration\n",
    "from models.bert.modeling_bert_v2 import BertForFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset !\n",
      "Read cache from cache/sighan_lattice_test.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_model_name_path=\"hfl/chinese-roberta-wwm-ext\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_model_name_path)\n",
    "\n",
    "\n",
    "datasets, vocabs, embedding = get_lattice_dataset(\"sighan\", path_head=\"../\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seq_len = max(*map(lambda x: max(x['seq_len']), datasets.values()))\n",
    "#max_lattice_len = 640\n",
    "config = BertConfig.from_pretrained(\n",
    "    \"hfl/chinese-roberta-wwm-ext\"\n",
    ")\n",
    "config.num_labels = len(vocabs['label'])\n",
    "config.vocab_size = len(vocabs['lattice'])\n",
    "config.max_seq_len = 128#max_seq_len\n",
    "config.max_lattice_len = 128#max_lattice_len\n",
    "config.bert_hidden_size = config.hidden_size\n",
    "config.hidden_size = 128#args.hidden_size\n",
    "config.num_attention_heads = 8\n",
    "config.rel_pos_init = 1\n",
    "config.learnable_position = False#args.learnable_position\n",
    "config.num_hidden_layers = 1#args.num_hidden_layers\n",
    "config.position_embedding_path = None#args.position_embedding_path\n",
    "config.use_crf = False#args.use_crf\n",
    "config.position_type = \"flat\"#args.position_type\n",
    "config.position_embedding = \"\"#args.position_embedding\n",
    "config.position_fusion = \"four\"#args.position_fusion\n",
    "config.position_first_layer = False#args.position_first_layer\n",
    "\n",
    "model = BertForFlat(config=config, word_embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[\"pos_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[\"target\"]))\n",
    "print(len(test_data[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73, 5286])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(8.6147, grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_data = datasets[\"train\"][0]\n",
    "\n",
    "def f(x):\n",
    "    return torch.tensor([x])\n",
    "\n",
    "model(\n",
    "    f(test_data[\"input_ids\"]),\n",
    "    f(test_data[\"atten_masks\"]),\n",
    "    f(test_data[\"pos_s\"]),\n",
    "    f(test_data[\"pos_e\"]), \n",
    "    f(test_data[\"target\"]), \n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5798d1b89faf9fad5f596318b99ae69e82e8b63ab102e599c8ecfb07b8dff05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dophin': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
