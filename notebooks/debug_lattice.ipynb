{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.fastNLP_module import StaticEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import cache_results\n",
    "\n",
    "\n",
    "@cache_results(_cache_fp='need_to_defined_fp',_refresh=True)\n",
    "def equip_chinese_ner_with_lexicon(datasets,vocabs,embeddings,w_list,word_embedding_path=None,\n",
    "                                   only_lexicon_in_train=False,word_char_mix_embedding_path=None,\n",
    "                                   number_normalized=False,\n",
    "                                   lattice_min_freq=1,only_train_min_freq=0):\n",
    "    from fastNLP.core import Vocabulary\n",
    "    def normalize_char(inp):\n",
    "        result = []\n",
    "        for c in inp:\n",
    "            if c.isdigit():\n",
    "                result.append('0')\n",
    "            else:\n",
    "                result.append(c)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def normalize_bigram(inp):\n",
    "        result = []\n",
    "        for bi in inp:\n",
    "            tmp = bi\n",
    "            if tmp[0].isdigit():\n",
    "                tmp = '0'+tmp[:1]\n",
    "            if tmp[1].isdigit():\n",
    "                tmp = tmp[0]+'0'\n",
    "\n",
    "            result.append(tmp)\n",
    "        return result\n",
    "\n",
    "    if number_normalized == 3:\n",
    "        for k,v in datasets.items():\n",
    "            v.apply_field(normalize_char,'chars','chars')\n",
    "        vocabs['char'] = Vocabulary()\n",
    "        vocabs['char'].from_dataset(datasets['train'], field_name='chars',\n",
    "                                no_create_entry_dataset=[datasets['dev'], datasets['test']])\n",
    "\n",
    "        for k,v in datasets.items():\n",
    "            v.apply_field(normalize_bigram,'bigrams','bigrams')\n",
    "        vocabs['bigram'] = Vocabulary()\n",
    "        vocabs['bigram'].from_dataset(datasets['train'], field_name='bigrams',\n",
    "                                  no_create_entry_dataset=[datasets['dev'], datasets['test']])\n",
    "\n",
    "\n",
    "    if only_lexicon_in_train:\n",
    "        print('已支持只加载在trian中出现过的词汇')\n",
    "\n",
    "    def get_skip_path(chars, w_trie):\n",
    "        sentence = ''.join(chars)\n",
    "        result = w_trie.get_lexicon(sentence)\n",
    "        # print(result)\n",
    "\n",
    "        return result\n",
    "    from V0.utils_ import Trie\n",
    "    from functools import partial\n",
    "    from fastNLP.core import Vocabulary\n",
    "    # from fastNLP.embeddings import StaticEmbedding\n",
    "    from fastNLP_module import StaticEmbedding\n",
    "    from fastNLP import DataSet\n",
    "    a = DataSet()\n",
    "    w_trie = Trie()\n",
    "    for w in w_list:\n",
    "        w_trie.insert(w)\n",
    "\n",
    "\n",
    "    if only_lexicon_in_train:\n",
    "        lexicon_in_train = set()\n",
    "        for s in datasets['train']['chars']:\n",
    "            lexicon_in_s = w_trie.get_lexicon(s)\n",
    "            for s,e,lexicon in lexicon_in_s:\n",
    "                lexicon_in_train.add(''.join(lexicon))\n",
    "\n",
    "        print('lexicon in train:{}'.format(len(lexicon_in_train)))\n",
    "        print('i.e.: {}'.format(list(lexicon_in_train)[:10]))\n",
    "        w_trie = Trie()\n",
    "        for w in lexicon_in_train:\n",
    "            w_trie.insert(w)\n",
    "\n",
    "    import copy\n",
    "    for k,v in datasets.items():\n",
    "        v.apply_field(partial(get_skip_path,w_trie=w_trie),'chars','lexicons')\n",
    "        v.apply_field(copy.copy, 'chars','raw_chars')\n",
    "        v.add_seq_len('lexicons','lex_num')\n",
    "        v.apply_field(lambda x:list(map(lambda y: y[0], x)), 'lexicons', 'lex_s')\n",
    "        v.apply_field(lambda x: list(map(lambda y: y[1], x)), 'lexicons', 'lex_e')\n",
    "\n",
    "\n",
    "    if number_normalized == 1:\n",
    "        for k,v in datasets.items():\n",
    "            v.apply_field(normalize_char,'chars','chars')\n",
    "        vocabs['char'] = Vocabulary()\n",
    "        vocabs['char'].from_dataset(datasets['train'], field_name='chars',\n",
    "                                no_create_entry_dataset=[datasets['dev'], datasets['test']])\n",
    "\n",
    "    if number_normalized == 2:\n",
    "        for k,v in datasets.items():\n",
    "            v.apply_field(normalize_char,'chars','chars')\n",
    "        vocabs['char'] = Vocabulary()\n",
    "        vocabs['char'].from_dataset(datasets['train'], field_name='chars',\n",
    "                                no_create_entry_dataset=[datasets['dev'], datasets['test']])\n",
    "\n",
    "        for k,v in datasets.items():\n",
    "            v.apply_field(normalize_bigram,'bigrams','bigrams')\n",
    "        vocabs['bigram'] = Vocabulary()\n",
    "        vocabs['bigram'].from_dataset(datasets['train'], field_name='bigrams',\n",
    "                                  no_create_entry_dataset=[datasets['dev'], datasets['test']])\n",
    "\n",
    "\n",
    "    def concat(ins):\n",
    "        chars = ins['chars']\n",
    "        lexicons = ins['lexicons']\n",
    "        result = chars + list(map(lambda x:x[2],lexicons))\n",
    "        return result\n",
    "\n",
    "    def get_pos_s(ins):\n",
    "        lex_s = ins['lex_s']\n",
    "        seq_len = ins['seq_len']\n",
    "        pos_s = list(range(seq_len)) + lex_s\n",
    "\n",
    "        return pos_s\n",
    "\n",
    "    def get_pos_e(ins):\n",
    "        lex_e = ins['lex_e']\n",
    "        seq_len = ins['seq_len']\n",
    "        pos_e = list(range(seq_len)) + lex_e\n",
    "\n",
    "        return pos_e\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        v.apply(concat,new_field_name='lattice')\n",
    "        v.set_input('lattice')\n",
    "        v.apply(get_pos_s,new_field_name='pos_s')\n",
    "        v.apply(get_pos_e, new_field_name='pos_e')\n",
    "        v.set_input('pos_s','pos_e')\n",
    "\n",
    "    word_vocab = Vocabulary()\n",
    "    word_vocab.add_word_lst(w_list)\n",
    "    vocabs['word'] = word_vocab\n",
    "\n",
    "    lattice_vocab = Vocabulary()\n",
    "    lattice_vocab.from_dataset(datasets['train'],field_name='lattice',\n",
    "                               no_create_entry_dataset=[v for k,v in datasets.items() if k != 'train'])\n",
    "    vocabs['lattice'] = lattice_vocab\n",
    "\n",
    "    if word_embedding_path is not None:\n",
    "        word_embedding = StaticEmbedding(word_vocab,word_embedding_path,word_dropout=0)\n",
    "        embeddings['word'] = word_embedding\n",
    "\n",
    "    if word_char_mix_embedding_path is not None:\n",
    "        lattice_embedding = StaticEmbedding(lattice_vocab, word_char_mix_embedding_path,word_dropout=0.01,\n",
    "                                            min_freq=lattice_min_freq,only_train_min_freq=only_train_min_freq)\n",
    "        embeddings['lattice'] = lattice_embedding\n",
    "\n",
    "    vocabs['char'].index_dataset(* (datasets.values()),\n",
    "                             field_name='chars', new_field_name='chars')\n",
    "    vocabs['bigram'].index_dataset(* (datasets.values()),\n",
    "                               field_name='bigrams', new_field_name='bigrams')\n",
    "    vocabs['label'].index_dataset(* (datasets.values()),\n",
    "                              field_name='target', new_field_name='target')\n",
    "    vocabs['lattice'].index_dataset(* (datasets.values()),\n",
    "                                    field_name='lattice', new_field_name='lattice')\n",
    "\n",
    "\n",
    "    return datasets,vocabs,embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets,vocabs,embeddings = equip_chinese_ner_with_lexicon(datasets,vocabs,embeddings,\n",
    "                                                            w_list,yangjie_rich_pretrain_word_path,\n",
    "                                                         _refresh=refresh_data,_cache_fp=cache_name,\n",
    "                                                         only_lexicon_in_train=args.only_lexicon_in_train,\n",
    "                                                            word_char_mix_embedding_path=yangjie_rich_pretrain_char_and_word_path,\n",
    "                                                            number_normalized=args.number_normalized,\n",
    "                                                            lattice_min_freq=args.lattice_min_freq,\n",
    "                                                            only_train_min_freq=args.only_train_min_freq)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5798d1b89faf9fad5f596318b99ae69e82e8b63ab102e599c8ecfb07b8dff05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dophin': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
