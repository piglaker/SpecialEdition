{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional,Dict, Union, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import trainer_utils, training_args\n",
    "from transformers.trainer_pt_utils import nested_detach\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from core import get_dataset, get_metrics, argument_init\n",
    "from lib import subTrainer  \n",
    "from data.DatasetLoadingHelper import load_ctc2021, load_sighan, load_lattice_sighan\n",
    "#from models.bart.modeling_bart_v2 import BartForConditionalGeneration\n",
    "from models.bert.modeling_bert_v2 import BertForFlat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_name_path=\"hfl/chinese-roberta-wwm-ext\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"今天天气不错\", \"今天天气还行哦\"]\n",
    "\n",
    "res = tokenizer.batch_encode_plus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 791, 1921, 1921, 3698, 679, 7231, 102],\n",
       "  [101, 791, 1921, 1921, 3698, 6820, 6121, 1521, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEncoding():\n",
    "    def __init__(self, ids, atten_masks, target, pos_s, pos_e):\n",
    "        self.ids = ids\n",
    "        self.atten_masks = atten_masks\n",
    "        self.target = target\n",
    "        self.pos_s = pos_s\n",
    "        self.pos_e = pos_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 791, 1921, 1921, 3698, 679, 7231, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['[CLS]', '今', '天', '天', '气', '不', '错', '[SEP]']\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (0, 0)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(res[0].ids)\n",
    "print(res[0].type_ids)\n",
    "print(res[0].tokens)\n",
    "print(res[0].offsets)\n",
    "print(res[0].attention_mask)\n",
    "#print(res[0].sequence_tokens_mask)\n",
    "#print(res[0].overflowing)\n",
    "res[0].n_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data_iter = [ { data[key][i] for key in data.keys() } for i in range(len(data[data.keys()[0]])) ]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.data[key]\n",
    "        else:   \n",
    "            return self.data_iter[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_offset(length):\n",
    "    res = []\n",
    "    res.append((0, 0))\n",
    "    for i in range(length-2):\n",
    "        res.append((i, i+1))\n",
    "    res.append((0,0))\n",
    "    return res\n",
    "\n",
    "#test\n",
    "get_offset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoding = Encoding(ids=[1], type_ids=[1], tokens=[\"是\"], offsets=[(0)], atten_masks=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=0, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = res.encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "son = dir(res.encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 791, 1921, 1921, 3698, 679, 7231, 102], [101, 791, 1921, 1921, 3698, 6820, 6121, 1521, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_encoding = Encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = BatchEncoding({\"input_ids\":[[1]], \"token_types_ids\":[[1]], \"attention_mask\":[[1]], \"pos_s\":[[1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finals = [[1],[2]]\n",
    "atten_masks = [[1], [2]]\n",
    "target = [[1], [2]]\n",
    "pos_s = [[1], [2]]\n",
    "pos_e = [[1], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import DataSet\n",
    "\n",
    "src = DataSet({ \"lattice\":finals, \"atten_masks\":atten_masks, \"target\": target, \"pos_s\":pos_s, \"pos_e\":pos_e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lattice', 'atten_masks', 'target', 'pos_s', 'pos_e'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.field_arrays.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+-------+-------+\n",
      "| lattice | atten_masks | target | pos_s | pos_e |\n",
      "+---------+-------------+--------+-------+-------+\n",
      "| [1]     | [1]         | [1]    | [1]   | [1]   |\n",
      "+---------+-------------+--------+-------+-------+\n",
      "+---------+-------------+--------+-------+-------+\n",
      "| lattice | atten_masks | target | pos_s | pos_e |\n",
      "+---------+-------------+--------+-------+-------+\n",
      "| [2]     | [2]         | [2]    | [2]   | [2]   |\n",
      "+---------+-------------+--------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "for i in src:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = {}\n",
    "\n",
    "for key in src.field_arrays:\n",
    "    new[key] = [i for i in src.field_arrays[key]]\n",
    "\n",
    "final = BatchEncoding(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lattice': [[1], [2]], 'atten_masks': [[1], [2]], 'target': [[1], [2]], 'pos_s': [[1], [2]], 'pos_e': [[1], [2]]} <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': [[101, 791, 1921, 1921, 3698, 679, 7231, 102], [101, 791, 1921, 1921, 3698, 6820, 6121, 1521, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]} <class 'transformers.tokenization_utils_base.BatchEncoding'> Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(final, type(final))\n",
    "print(res, type(res), res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lattice': [[1], [2]], 'atten_masks': [[1], [2]], 'target': [[1], [2]], 'pos_s': [[1], [2]], 'pos_e': [[1], [2]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': {'lattice': [[1], [2]], 'atten_masks': [[1], [2]], 'target': [[1], [2]], 'pos_s': [[1], [2]], 'pos_e': [[1], [2]]},\n",
       " 'test': {'lattice': [[1], [2]], 'atten_masks': [[1], [2]], 'target': [[1], [2]], 'pos_s': [[1], [2]], 'pos_e': [[1], [2]]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {\"train\":src, \"test\":src}\n",
    "\n",
    "def tmp_transform(fnlp_dataset):\n",
    "    new = {}\n",
    "\n",
    "    for key in fnlp_dataset.field_arrays:\n",
    "        new[key] = [i for i in fnlp_dataset.field_arrays[key]]\n",
    "\n",
    "    res = BatchEncoding(new)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "test = tmp_transform(dataset[\"train\"])\n",
    "\n",
    "print(test)\n",
    "\n",
    "dataset2 = dict(zip(dataset, map(tmp_transform, dataset.values())))\n",
    "\n",
    "\n",
    "dataset2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5798d1b89faf9fad5f596318b99ae69e82e8b63ab102e599c8ecfb07b8dff05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dophin': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
