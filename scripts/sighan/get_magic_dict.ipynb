{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import jieba\n",
    "\n",
    "sys.path.append(\"../../\") \n",
    "from utils.io import read_csv, write_to, load_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "#\n",
    "def preprocess(sentence):\n",
    "    s = strQ2B(sentence)\n",
    "    back_num = re.findall('\\d+', s)\n",
    "    back_eng = re.findall(r'[a-zA-Z]+', s)\n",
    "    s = re.sub(r'[a-zA-Z]+', 'e', s)\n",
    "    s = re.sub('\\d+', 'n', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sighan_from_json():\n",
    "\n",
    "    all_data = {\n",
    "        \"train\":None,\n",
    "        \"dev\":None,\n",
    "        \"test\":None,\n",
    "        \"test14\":None,\n",
    "        \"test15\":None,\n",
    "    }\n",
    "    data_dir = \"../../data/rawdata/sighan/csc/\"\n",
    "\n",
    "    train_file1 = os.path.join(data_dir, \"train_dev.json\")\n",
    "    train_file2 = os.path.join(data_dir, \"train131415.json\") \n",
    "    test14_file = os.path.join(data_dir, \"test14.json\")\n",
    "    test15_file = os.path.join(data_dir, \"test15.json\")\n",
    "\n",
    "    all_data[\"train\"] = load_json(train_file1)\n",
    "    all_data[\"train\"].extend(load_json(train_file2))\n",
    "\n",
    "    all_data[\"train\"] = all_data[\"train\"]\n",
    "\n",
    "    all_data[\"valid14\"] = load_json(test14_file)\n",
    "    all_data[\"valid\"] = load_json(test15_file)\n",
    "    #all_data[\"test\"].extend(load_json(test15_file))\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_preprocess(sentence):\n",
    "    import re\n",
    "    import jieba\n",
    "    return [ i for i in jieba.cut(re.sub(\"\\W*\", \"\", sentence)) if len(i) >= 1] \n",
    "\n",
    "def json2list(data, need_preprocess):\n",
    "    source, target, ids = [], [], []\n",
    "\n",
    "    for element in data:\n",
    "\n",
    "        if need_preprocess:\n",
    "            source.append(preprocess(element[\"original_text\"]))\n",
    "            target.append(preprocess(element[\"correct_text\"]))\n",
    "            ids.apoend(element[\"wrong_ids\"])\n",
    "        else:\n",
    "            source.append(strQ2B((element[\"original_text\"])))\n",
    "            target.append(strQ2B((element[\"correct_text\"])))\n",
    "            ids.append(element[\"wrong_ids\"])\n",
    "\n",
    "    return source, target, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_sighan_from_json()\n",
    "\n",
    "train_source, train_target, train_ids = json2list(data[\"train\"], False)\n",
    "\n",
    "valid14_source, valid14_target, valid14_ids = json2list(data[\"valid14\"], False)\n",
    "\n",
    "valid_source, valid_target, valid_ids = json2list(data[\"valid\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_train = train_source + valid14_source + valid_source\n",
    "\n",
    "all_target = train_target + valid14_target + valid_target\n",
    "\n",
    "all_ids = train_ids + valid14_ids + valid_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284196it [00:03, 71200.20it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_all_cor(train_source_target_ids):  \n",
    "    train_cor = {}\n",
    "    train_cor_graph = {}\n",
    "    from tqdm import tqdm\n",
    "    for source, target, ids in tqdm(train_source_target_ids):\n",
    "        for i in range(len(source)):\n",
    "            if source[i] != target[i]:\n",
    "                train_cor[(source[i], target[i])] = 0\n",
    "                if source[i] in train_cor_graph:\n",
    "                    train_cor_graph[source[i]][target[i]] = 0\n",
    "                else:\n",
    "                    train_cor_graph[source[i]] = {} \n",
    "\n",
    "    return train_cor, train_cor_graph\n",
    "\n",
    "all_cor, cor_graph = get_all_cor(zip(train_source, train_target, train_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "菖 {'普': 0, '遭': 0, '营': 0, '直': 0, '富': 0, '算': 0, '百': 0, '葛': 0, '雷': 0, '害': 0, '晶': 0, '雪': 0, '鲁': 0, '警': 0, '蓄': 0, '喜': 0, '看': 0, '首': 0, '誓': 0, '暂': 0, '真': 0, '薄': 0, '蔓': 0, '着': 0, '董': 0, '享': 0, '掌': 0, '笔': 0, '者': 0, '副': 0, '毫': 0, '省': 0, '音': 0, '拿': 0, '署': 0, '落': 0, '信': 0, '置': 0, '言': 0, '革': 0, '童': 0, '肇': 0, '查': 0, '尊': 0, '囊': 0, '菁': 0, '蕾': 0, '章': 0, '霭': 0, '翼': 0, '事': 0, '寡': 0, '宴': 0, '莤': 0, '雾': 0, '苦': 0, '背': 0, '莒': 0, '籍': 0, '宣': 0, '青': 0, '蕃': 0, '暨': 0, '餐': 0, '苫': 0, '管': 0, '夏': 0, '霉': 0, '督': 0, '豆': 0, '震': 0}\n",
      "熹 {'嘉': 0, '景': 0, '需': 0, '害': 0, '置': 0, '率': 0, '拿': 0, '靠': 0, '案': 0, '悬': 0, '蒙': 0, '鼐': 0, '喜': 0, '畜': 0, '纂': 0, '影': 0, '表': 0, '寄': 0, '幕': 0, '亭': 0, '毫': 0, '菁': 0, '善': 0, '甚': 0, '责': 0, '点': 0, '事': 0, '慕': 0, '蠢': 0, '蔓': 0, '海': 0, '享': 0, '寮': 0, '美': 0, '震': 0, '纛': 0, '桑': 0, '哥': 0, '煮': 0, '黛': 0, '盲': 0, '亮': 0, '筹': 0, '烹': 0, '信': 0, '叠': 0, '蒂': 0, '丢': 0, '蓄': 0, '囊': 0, '鬣': 0, '蕉': 0, '誓': 0, '募': 0, '索': 0, '觐': 0, '蕾': 0, '鼻': 0, '蒸': 0, '希': 0}\n",
      "[71, 60, 59, 48, 46, 46, 42, 42, 41, 38, 37, 37, 37, 37, 36, 36, 35, 34, 34, 33, 33, 32, 32, 32, 31, 31, 31, 30, 30, 30, 30, 30, 30, 29, 29, 29, 28, 28, 28, 28, 28, 28, 28, 27, 27, 26, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for key, values in cor_graph.items():\n",
    "\n",
    "    length.append(len(values))\n",
    "    if len(values) >= 60:\n",
    "        print(key, values)\n",
    "\n",
    "\n",
    "length.sort(reverse=True)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1100it [00:00, 94934.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "valid_cor = get_all_cor(zip(valid_source, valid_target, valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('户', '糊')\n",
      "('的', '名')\n",
      "('码', '么')\n",
      "('么', '没')\n",
      "('增', '噌')\n",
      "('乍', '怎')\n",
      "('枝', '只')\n",
      "('鲙', '怪')\n",
      "('卓', '著')\n",
      "('暂', '占')\n",
      "('床', '墙')\n",
      "0.9762419006479481\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in valid_cor.keys():\n",
    "    if key in all_cor:\n",
    "        count += 1\n",
    "    else:\n",
    "        print(key)\n",
    "print(count / len(valid_cor.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4027272727272727\n"
     ]
    }
   ],
   "source": [
    "#能被分词(长度>2的错误)\n",
    "def count_wrongs(all_data_and_ids):  \n",
    "    count = 0\n",
    "\n",
    "    for data, ids in all_data_and_ids:\n",
    "\n",
    "        if not ids:\n",
    "            continue\n",
    "\n",
    "        spans = [ i for i in jieba.cut(data) ]\n",
    "\n",
    "        pointer_ids = 0\n",
    "    \n",
    "        pointer = 0\n",
    "\n",
    "        for span in spans:\n",
    "\n",
    "            pointer += len(span)\n",
    "        \n",
    "            tmp = \"\"\n",
    "\n",
    "            if pointer >= ids[pointer_ids]:\n",
    "\n",
    "                if not span.isdigit() and len(span) != 1:\n",
    "                    tmp = span\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                pointer_ids += 1\n",
    "                if pointer_ids >= len(ids):\n",
    "                    break\n",
    "\n",
    "    return count\n",
    "\n",
    "t = count_wrongs(zip(valid_target, valid_ids))\n",
    "\n",
    "print(t / len(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988043462962181\n",
      "0.49272727272727274\n"
     ]
    }
   ],
   "source": [
    "#count 训练和test,要改的句子数量的样本不均衡\n",
    "train_wrongs = 0\n",
    "for i in train_ids:\n",
    "    if i:\n",
    "        train_wrongs += 1\n",
    "print(train_wrongs / len(train_ids))\n",
    "\n",
    "test_wrongs = 0\n",
    "for j in valid_ids:\n",
    "    if j:\n",
    "        test_wrongs += 1\n",
    "print(test_wrongs / len(valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4027272727272727\n"
     ]
    }
   ],
   "source": [
    "t_test = count_wrongs(zip(valid_target, valid_ids))\n",
    "print(t_test / len(valid_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset !\n",
      "Mon Dec 13 11:49:13 UTC 2021\n",
      "Loading Lattice SigHan Dataset ...\n",
      "Found 5601 out of 5671 words in the pre-training embedding.\n",
      "Save cache to cache/sighan_lattice_test.\n"
     ]
    }
   ],
   "source": [
    "#let's check the dataset hit status\n",
    "sys.path.append(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition\")\n",
    "\n",
    "from core import get_lattice_dataset\n",
    "\n",
    "datasets, vocabs, embeddings = get_lattice_dataset(dataset=\"sighan\", path_head=\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396220 284248 0.7173994245621119\n"
     ]
    }
   ],
   "source": [
    "total_wrongs = 0\n",
    "hit = 0\n",
    "\n",
    "for i in range(len(train_target)):\n",
    "    wrong_char_host =[]\n",
    "    for j in range(len(train_target[i])):\n",
    "        if train_target[i][j] != train_source[i][j]:\n",
    "            wrong_char_host.append(train_target[i][j])\n",
    "\n",
    "    arrows = datasets['train'][i]['lattice'][(datasets['train'][i]['seq_len'] - datasets['train'][i]['lex_nums']):]\n",
    "\n",
    "    for wrong_char in wrong_char_host:\n",
    "        if wrong_char in arrows:\n",
    "            hit += 1\n",
    "\n",
    "    total_wrongs += len(wrong_char_host)\n",
    "\n",
    "print(total_wrongs, hit, hit / total_wrongs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705 396 0.5617021276595745\n"
     ]
    }
   ],
   "source": [
    "total_wrongs = 0\n",
    "hit = 0\n",
    "\n",
    "for i in range(len(valid_target)):\n",
    "    wrong_char_host =[]\n",
    "    for j in range(len(valid_target[i])):\n",
    "        if valid_target[i][j] != valid_source[i][j]:\n",
    "            wrong_char_host.append(valid_target[i][j])\n",
    "\n",
    "    arrows = datasets['test'][i]['lattice'][(datasets['test'][i]['seq_len'] - datasets['test'][i]['lex_nums']):]\n",
    "\n",
    "    for wrong_char in wrong_char_host:\n",
    "        if wrong_char in arrows:\n",
    "            hit += 1\n",
    "\n",
    "    total_wrongs += len(wrong_char_host)\n",
    "\n",
    "print(total_wrongs, hit, hit / total_wrongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'西信设看道到返基基基事师市节木兰立报保古光门富关管馆班结金塞蒙金塞吉新溪背迪多汉塔际机结上击几旧如时兴语连联班结金塞蒙金塞吉已忆引记己次七气汽期齐唐哈里'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(datasets['train'][0]['lattice'][(datasets['train'][0]['seq_len'] - datasets['train'][0]['lex_nums']):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法新社记者报导,法国驻巴蓦斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\n",
      "法新社记者报导,法国驻巴基斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\n",
      "[12]\n",
      "['法新社', '记者', '报导', ',', '法国', '驻', '巴基斯坦', '大使', '杰拉德', ',', '以及', '巴国', '官员', '在', '巴基斯坦', '西北部', '的', '托克', '哈姆', '边界', '关卡', '迎接', '十月', '九日', '与', '两名', '巴基斯坦', '同业', '一起', '遭', '塔利班', '逮捕', '的', '裴哈', '。']\n",
      "巴基斯坦\n"
     ]
    }
   ],
   "source": [
    "print(train_source[0])\n",
    "print(all_target[0])\n",
    "print(all_ids[0])\n",
    "\n",
    "tmp = [ i for i in jieba.cut(all_target[0])]\n",
    "\n",
    "print(tmp)\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "for target in all_target:\n",
    "    tmp = [ i for i in jieba.cut(target) ]\n",
    "    pointer = 0\n",
    "    for j in tmp:\n",
    "        pointer += len(j)\n",
    "        if pointer >= all_ids[0][0]:\n",
    "            result = j\n",
    "            break\n",
    "    break\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572716/572716 [00:04<00:00, 140392.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5723\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "all_char_list = []\n",
    "for sentence in tqdm(all_train + all_target):\n",
    "    tmp = [j for j in sentence]\n",
    "    all_char_list += tmp\n",
    "\n",
    "all_char_set = list(set(all_char_list))\n",
    "print(len(all_char_set))\n",
    "\n",
    "write_to(\"my_char_dict.txt\", \"\\n\".join(all_char_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to(\"my_char_dict.txt\", \"\\n\".join(all_char_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_word(data, ids):\n",
    "    \"\"\"\n",
    "            法新社记者报导,法国驻巴蓦斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\n",
    "    data : \"法新社记者报导,法国驻巴基斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\"\n",
    "    \n",
    "    ids: [12]\n",
    "    \"\"\"\n",
    "    if not ids:\n",
    "        return \"\"\n",
    "\n",
    "    spans = [ i for i in jieba.cut(data)]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    pointer = 0\n",
    "\n",
    "    for span in spans:   \n",
    "\n",
    "        pointer += len(span)\n",
    "        \n",
    "        tmp = \"\"\n",
    "        span = re.sub(\"\\W*\", \"\", span)\n",
    "        if span and not span.isdigit() and len(span) != 1:\n",
    "            tmp = span\n",
    "            \n",
    "        if tmp:\n",
    "            result.append(tmp)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286358it [02:04, 2307.62it/s]\n"
     ]
    }
   ],
   "source": [
    "all_word_dict_plus = {}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for data, id in tqdm(zip(all_target, all_ids)):\n",
    "    for i in get_all_word(data, id):\n",
    "        all_word_dict_plus[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to(\"all_word_dict.txt\", \"\\n\".join([i for i in all_word_dict_plus.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magic_word(data, ids):\n",
    "    \"\"\"\n",
    "            法新社记者报导,法国驻巴蓦斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\n",
    "    data : \"法新社记者报导,法国驻巴基斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\"\n",
    "    \n",
    "    ids: [12]\n",
    "    \"\"\"\n",
    "    #print(data, ids)\n",
    "    if not ids:\n",
    "        return \"\"\n",
    "\n",
    "    spans = [ i for i in jieba.cut(data) ]\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    pointer_ids = 0\n",
    "    \n",
    "    pointer = 0\n",
    "    #print(spans)\n",
    "    for span in spans:\n",
    "\n",
    "        pointer += len(span)\n",
    "        \n",
    "        tmp = \"\"\n",
    "        if pointer > ids[pointer_ids]:\n",
    "            span = re.sub(\"\\W*\", \"\", span)\n",
    "            if span and not span.isdigit():\n",
    "                tmp = span\n",
    "            \n",
    "            if tmp:\n",
    "                result.append(tmp)\n",
    "\n",
    "            pointer_ids += 1\n",
    "            if pointer_ids >= len(ids):\n",
    "                break\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_magic_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81906/1484726994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmagic_dict\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_magic_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_magic_word' is not defined"
     ]
    }
   ],
   "source": [
    "magic_dict = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for data, id in tqdm(zip(valid_target, valid_ids)):\n",
    "    magic_dict += get_magic_word(data, id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'magic_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81906/439143634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'magic_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(magic_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in valid_ids:\n",
    "    if i :\n",
    "        c += len(i)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "681 477\n"
     ]
    }
   ],
   "source": [
    "print(len(magic_dict))\n",
    "count_word = 0\n",
    "count_ =0\n",
    "\n",
    "for i in magic_dict:\n",
    "    #\n",
    "    count_ += 1\n",
    "    if len(i) > 1:\n",
    "        count_word += 1\n",
    "print(count_, count_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291269/291269 [00:01<00:00, 182904.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "import jieba\n",
    "\n",
    "sys.path.append(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition\")\n",
    "\n",
    "from utils.io import read_csv, write_to, load_json\n",
    "\n",
    "from utils.trie_utils import list2confusion_trie\n",
    "\n",
    "confusion_set =  read_csv(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/confusion_set/confusion.txt\")\n",
    "\n",
    "confusion_dict = {}\n",
    "\n",
    "for line in confusion_set:\n",
    "\tline = line.split(\":\")\n",
    "\tconfusion_dict[line[0][0]] = line[-1]\n",
    "\n",
    "all_word_list = read_csv(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/scripts/sighan/all_word_dict.txt\")\n",
    "\n",
    "def wash_n(all_word_list):\n",
    "\treturn [ re.sub(\"\\n\", \"\", i) for i in all_word_list]\n",
    "\n",
    "all_word_list = wash_n(all_word_list)\n",
    "all_word_dict = {i:0 for i in all_word_list}\n",
    "trie = list2confusion_trie(all_word_list, confusion_dict)\n",
    "\n",
    "def super_get(sentence):\n",
    "\ttrie.assign(sentence)\n",
    "\ttrie.my_get_lexion()\n",
    "\treturn trie.result\n",
    "\n",
    "path = \"./30wdict_utf8.txt\"\n",
    "dict_ = read_csv(path)\n",
    "word_dict = {}\n",
    "from tqdm import tqdm\n",
    "for line in tqdm(dict_):\n",
    "    word_dict[re.sub(\"\\W*\", \"\", line)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_word_dict = []\n",
    "\n",
    "for sentence in bert_missed_source:#valid_source:\n",
    "    tmp = super_get(sentence)\n",
    "    tmp = [ i[-1] for i in tmp]\n",
    "\n",
    "    matched_word_dict += tmp\n",
    "\n",
    "matched_word_dict = {i:0 for i in set(matched_word_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_missed = read_csv(\"bert_missed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n"
     ]
    }
   ],
   "source": [
    "print(len(bert_missed) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_missed_source = []\n",
    "i = 0\n",
    "for s in bert_missed:\n",
    "    if i % 3 == 0:\n",
    "        bert_missed_source.append(s)\n",
    "    i+= 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(bert_missed_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "need = ['小鸡', '记得', '糊涂', '订位', '怎么', '面见', '弟弟', '炒饭', '汉字', '那店', '八点钟', '这样', '迟到', '很饱', '迟到', '迟到', '安静', '一点', '妈妈', '吃饭', '时候', '希望', '简讯', '那里', '诉取', '不半工', '哪些', '上半', '上半', '轻松', '消息', '与此同时', '这种', '权利', '优家', '失业', '心情', '减少', '著迷', '家长', '唠叨', '影响', '睡觉', '拜托', '存到', '年轻', '威胁', '我们', '回覆', '墙壁', '他们', '这是', '现在', '很忙', '不怎么', '试试看', '录影机', '哪个', '哪个', '不是', '再加', '该不该', '看著', '看著', '有装', '教书', '记录']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:\n",
      "Missed : 17\n",
      "50 50\n",
      "Set:\n",
      "Missed : 16\n",
      "46 46\n"
     ]
    }
   ],
   "source": [
    "print(\"List:\")\n",
    "count = 0\n",
    "count_super = 0\n",
    "missed = []\n",
    "for word in need:#magic_dict:\n",
    "    if word in matched_word_dict:\n",
    "        count_super += 1\n",
    "    if word in all_word_dict and word in matched_word_dict:\n",
    "        count += 1\n",
    "    if len(word) != 1 and word not in matched_word_dict:\n",
    "        missed.append(word)\n",
    "\n",
    "print(\"Missed :\", len(missed))\n",
    "print(count_super, count)\n",
    "\n",
    "print(\"Set:\")\n",
    "count = 0\n",
    "count_super = 0\n",
    "missed = []\n",
    "for word in list(set(need)):#list(set(magic_dict)):\n",
    "    if word in matched_word_dict:\n",
    "        count_super += 1\n",
    "    if word in all_word_dict and word in matched_word_dict:\n",
    "        count += 1\n",
    "    if len(word) != 1 and word not in matched_word_dict:\n",
    "        missed.append(word)\n",
    "\n",
    "print(\"Missed :\", len(missed))\n",
    "print(count_super, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['法新社记者报导,法国驻巴蓦斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。', '冰皂与挪威北部间数百公鳕宽的区域均为危险区。', '孟加拉官员及撮导今天说,过去两周至少有四十七人,大多为儿童,死于污染与季节变化所引发的呼吸道疾病及腹泻。', '本月十一日,西班牙首都马德里发生严重伤亡的爆炸事件,造咸两百人丧生和一千四百余人轻重伤。', '毕兰德拉对王室统治的见解,鹿信与新王完全泪反。', '玩游戏以及阅读电子枢方面。', '华府市鲁金斯研究所政治分析家海斯说,未来的国家元首面对国会时应更加审慎。', '但是,撮导推测,金正日可能已前往西伯利亚伊尔库次克途中,准备在当地与陇罗斯总统蒲亭举行会谈。', '他目前于誓方戒护下在医院接受治疗。', '布希上周三出席全代会时说,在阿富汗境内与美国并肩作战的国家约苜四十个,而在伊拉克境内则有三十个。', '差了几次牌子也无计于事。', '她说,这艘船预定在科林斯卸载一万两千公吨的基改大豆和七干公吨的墓改玉米。', '即将在一个月后离任的柯林顿,此次是他以美国总统身分第三度也是最后一次赴北爱从事官方访问。他在动荡不安的北爱,鼓利当地人民勿放气对持久和平方案的最求。', '在金拥春发表这项演说前,华盛顿当局正针对北韩杯葛朝鲜半岛核子问题六方会谈采取越来越强硬的立场。', '哈利在着名的伊顿学院修习了艺术和地理课程,成绩分别为和。级课程的评分由到,以为最高分。他的父亲查尔斯王子说:对哈利,我感到非常骄傲他很用公准备考试,我对今天的成绩感到很高兴。'] ['法新社记者报导,法国驻巴基斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。', '冰岛与挪威北部间数百公里宽的区域均为危险区。', '孟加拉官员及报导今天说,过去两周至少有四十七人,大多为儿童,死于污染与季节变化所引发的呼吸道疾病及腹泻。', '本月十一日,西班牙首都马德里发生严重伤亡的爆炸事件,造成两百人丧生和一千四百余人轻重伤。', '毕兰德拉对王室统治的见解,咸信与新王完全相反。', '玩游戏以及阅读电子书方面。', '华府布鲁金斯研究所政治分析家海斯说,未来的国家元首面对国会时应更加审慎。', '但是,报导推测,金正日可能已前往西伯利亚伊尔库次克途中,准备在当地与俄罗斯总统蒲亭举行会谈。', '他目前于警方戒护下在医院接受治疗。', '布希上周三出席全代会时说,在阿富汗境内与美国并肩作战的国家约有四十个,而在伊拉克境内则有三十个。', '插了几次牌子也无济于事。', '她说,这艘船预定在科林斯卸载一万两千公吨的基改大豆和七千公吨的基改玉米。', '即将在一个月后离任的柯林顿,此次是他以美国总统身分第三度也是最后一次赴北爱从事官方访问。他在动荡不安的北爱,鼓励当地人民勿放弃对持久和平方案的追求。', '在金永春发表这项演说前,华盛顿当局正针对北韩杯葛朝鲜半岛核子问题六方会谈采取越来越强硬的立场。', '哈利在着名的伊顿学院修习了艺术和地理课程,成绩分别为和。级课程的评分由到,以为最高分。他的父亲查尔斯王子说:对哈利,我感到非常骄傲他很用功准备考试,我对今天的成绩感到很高兴。']\n"
     ]
    }
   ],
   "source": [
    "print(train_source[:15], all_target[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12], [1, 11], [6], [27], [13, 20], [9], [2], [3, 34], [4], [30], [0, 8], [27, 31], [55, 62, 71], [2], [68]]\n"
     ]
    }
   ],
   "source": [
    "print(all_ids[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['巴基斯坦', '冰岛', '咸信', '相反', '电子书', '华府', '报导', '俄罗斯', '基改', '当地', '持久和平', '金永春']\n"
     ]
    }
   ],
   "source": [
    "print(magic_dict[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42694\n"
     ]
    }
   ],
   "source": [
    "print(len(magic_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to(\"./magic_dict.txt\", \"\\n\".join(magic_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2d(word_dict):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for word in word_dict:\n",
    "        key = len(word)\n",
    "        if key in d:\n",
    "            d[key].append(word)\n",
    "        else:\n",
    "            d[key] = [word]\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_dict = read_csv(\"./magic_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wash(magic_dict):\n",
    "    result = []\n",
    "    for i in magic_dict:\n",
    "        tmp = re.sub(\"\\n\", \"\", i)\n",
    "        if tmp:\n",
    "            result.append(tmp)\n",
    "    return result\n",
    "\n",
    "magic_dict = wash(magic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_better_dict(word_dict):\n",
    "    d = {}\n",
    "\n",
    "    for word in word_dict:\n",
    "        key_1 = len(word)\n",
    "\n",
    "        #small -> ignore length==2\n",
    "        if len(word) == 2:\n",
    "            continue\n",
    "\n",
    "        if key_1 in d.keys():\n",
    "            key_2 = word[0]\n",
    "            key_3 = word[-1]\n",
    "\n",
    "            if key_2 in d[key_1][0].keys():\n",
    "                d[key_1][0][key_2].append(word)\n",
    "            else:\n",
    "                d[key_1][0][key_2] = [word]\n",
    "\n",
    "            if key_3 in d[key_1][-1].keys():\n",
    "                d[key_1][-1][key_3].append(word)\n",
    "            else:\n",
    "                d[key_1][-1][key_3] = [word]\n",
    "        else:\n",
    "            d[key_1] = {0:{},-1:{} }\n",
    "            key_2 = word[0]\n",
    "            key_3 = word[-1]\n",
    "            \n",
    "            d[key_1][0][key_2] = [word]\n",
    "\n",
    "            d[key_1][-1][key_3] = [word]\n",
    "            \n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_better_dict(magic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 6, 4, 5, 7, 8, 9, 11, 10, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[3]\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_match(data, d):\n",
    "\n",
    "    result, map_info = [], []\n",
    "\n",
    "    index = 0 \n",
    "\n",
    "    #print(data)\n",
    "\n",
    "    for word in data:\n",
    "\n",
    "        if len(word) not in d.keys():\n",
    "            index += len(word) \n",
    "            continue\n",
    "\n",
    "        length = len(word)\n",
    "\n",
    "        key_left, key_right = word[0], word[-1]\n",
    "\n",
    "        if key_left in d[length][0]:\n",
    "            d_left = d[length][0][word[0]]\n",
    "        else:\n",
    "            d_left = {}\n",
    "\n",
    "        if key_right in d[length][-1]:\n",
    "            d_right = d[length][-1][word[-1]]\n",
    "        else:\n",
    "            d_right = {}\n",
    "\n",
    "        pair = []\n",
    "\n",
    "        for query in d_right:\n",
    "            count = 0\n",
    "            \n",
    "            for i in range(len(query)):\n",
    "                if count > 1:\n",
    "                    break\n",
    "                elif query[i] == word[i]:\n",
    "                    pass\n",
    "                else:\n",
    "                    count += 1\n",
    "            if count == 1:\n",
    "                pair.append(query)\n",
    "                map_info.append(\"$\".join(list(map(str, range(index, index+len(word))))))\n",
    "            \n",
    "        for query in d_left:\n",
    "            count = 0\n",
    "            \n",
    "            for i in range(len(query)):\n",
    "                if count > 1:\n",
    "                    break\n",
    "                elif query[i] == word[i]:\n",
    "                    pass\n",
    "                else:\n",
    "                    count += 1\n",
    "            if count == 1:\n",
    "                if query not in pair:\n",
    "                    pair.append(query)\n",
    "                    map_info.append(\"$\".join(list(map(str, range(index, index+len(word))))))\n",
    "\n",
    "        index += length\n",
    "        result += pair\n",
    "        \n",
    "    return result, map_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def air_preprocess(sentence):\n",
    "    import re\n",
    "    import jieba\n",
    "    return [ i for i in jieba.cut(sentence) if len(i) >= 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好!我是张爱文。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['张书文'], ['5$6$7'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_source[0])\n",
    "better_match(air_preprocess(valid_source[0]), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284196/284196 [02:58<00:00, 1590.44it/s]\n",
      "100%|██████████| 1062/1062 [00:00<00:00, 2012.29it/s]\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 3280.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.5640344619751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for data in tqdm(train_source):\n",
    "    result.append(better_match(light_preprocess(data), d))    \n",
    "    \n",
    "result_valid14 = []\n",
    "\n",
    "for data in tqdm(valid14_source):\n",
    "    result_valid14.append(better_match(light_preprocess(data), d))  \n",
    "\n",
    "result_valid = []\n",
    "\n",
    "for data in tqdm(valid_source):\n",
    "    result_valid.append(better_match(light_preprocess(data), d))  \n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['中新社',\n",
       "  '法新电',\n",
       "  '罗斯坦',\n",
       "  '杜斯坦',\n",
       "  '康斯坦',\n",
       "  '普拉德',\n",
       "  '巴勒斯坦',\n",
       "  '东北部',\n",
       "  '西南部',\n",
       "  '巴勒斯坦',\n",
       "  '塔利亚'],\n",
       " ['0$1$2',\n",
       "  '0$1$2',\n",
       "  '11$12$13',\n",
       "  '11$12$13',\n",
       "  '11$12$13',\n",
       "  '16$17$18',\n",
       "  '25$26$27$28',\n",
       "  '29$30$31',\n",
       "  '29$30$31',\n",
       "  '48$49$50$51',\n",
       "  '56$57$58'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.586109586341821"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, result[2]))\n",
    "sum(map(lambda x:sum(map(len, x)), result)) / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([], []), ([], []), (['联合体', '联合党', '联合会', '联合报', '对产龄', '生产率', '生育力', '生育期'], ['2$3$4', '2$3$4', '2$3$4', '2$3$4', '26$27$28', '33$34$35', '33$34$35', '33$34$35']), (['为什么'], ['6$7$8']), (['联合体', '联合党', '联合会', '联合报', '生产率', '生育力', '生育期'], ['2$3$4', '2$3$4', '2$3$4', '2$3$4', '33$34$35', '33$34$35', '33$34$35'])]\n",
      "[([], []), ([], []), (['联合体', '联合党', '联合会', '联合报', '对产龄', '生产率', '生育力', '生育期'], ['2$3$4', '2$3$4', '2$3$4', '2$3$4', '26$27$28', '33$34$35', '33$34$35', '33$34$35']), (['为什么'], ['6$7$8']), (['联合体', '联合党', '联合会', '联合报', '生产率', '生育力', '生育期'], ['2$3$4', '2$3$4', '2$3$4', '2$3$4', '33$34$35', '33$34$35', '33$34$35'])]\n"
     ]
    }
   ],
   "source": [
    "print(result_valid14[:5]) \n",
    "print(result_valid14[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['张书文'], ['2$3$4']), (['上个星期'], ['0$1$2$3']), ([], []), ([], []), ([], [])]\n",
      "[(['张书文'], ['2$3$4']), (['上个星期'], ['0$1$2$3']), ([], []), ([], []), ([], [])]\n"
     ]
    }
   ],
   "source": [
    "print(result_valid[:5]) \n",
    "print(result_valid[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2$3$4$5 6$7$8 6$7$8 6$7$8 6$7$8 6$7$8 6$7$8 11$12$13'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(list(map(\"\".join, result[6][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284196\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def app(train_source, result):\n",
    "    new = []\n",
    "    for i in range(len(train_source)):\n",
    "        tmp = train_source[i] + \"\\n\" + \" \".join(result[i][0]) + \",\" + \" \".join(result[i][1])\n",
    "        new.append(tmp)\n",
    "    return new\n",
    "\n",
    "train_magic_source, valid14_magic_source, valid_magic_source = app(train_source, result), app(valid14_source, result_valid14), app(valid_source, result_valid)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法新社记者报导,法国驻巴蓦斯坦大使杰拉德,以及巴国官员在巴基斯坦西北部的托克哈姆边界关卡迎接十月九日与两名巴基斯坦同业一起遭塔利班逮捕的裴哈。\n",
      "中新社 法新电 罗斯坦 杜斯坦 康斯坦 普拉德 巴勒斯坦 东北部 西南部 巴勒斯坦 塔利亚,0$1$2 0$1$2 11$12$13 11$12$13 11$12$13 16$17$18 25$26$27$28 29$30$31 29$30$31 48$49$50$51 56$57$58\n",
      "相对的、每位产龄妇女的生育婴儿个数却特续下滑。这表示全球出现适合年龄生育的妇女不想生育的现象。\n",
      ",\n",
      "你好!我是张爱文。\n",
      "张书文,2$3$4\n"
     ]
    }
   ],
   "source": [
    "print(train_magic_source[0])\n",
    "print(valid14_magic_source[0])\n",
    "print(valid_magic_source[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_magic_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b05d84f469b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/train.src\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_magic_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwrite_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/train.tgt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwrite_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid14.src\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid14_magic_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwrite_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid14.tgt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid14_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_magic_source' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/train.src\", \"\\n\".join(train_magic_source))\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/train.tgt\", \"\\n\".join(train_target))\n",
    "\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid14.src\", \"\\n\".join(valid14_magic_source))\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid14.tgt\", \"\\n\".join(valid14_target))\n",
    "\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/test.src\", \"\\n\".join(valid_magic_source))\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/test.tgt\", \"\\n\".join(valid_target))\n",
    "\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid.src\", \"\\n\".join(valid_magic_source[500:]))\n",
    "write_to(\"/remote-home/xtzhang/CTC/CTC2021/SpecialEdition/data/rawdata/sighan/lattice/valid.tgt\", \"\\n\".join(valid_target[500:]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5798d1b89faf9fad5f596318b99ae69e82e8b63ab102e599c8ecfb07b8dff05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dophin': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
